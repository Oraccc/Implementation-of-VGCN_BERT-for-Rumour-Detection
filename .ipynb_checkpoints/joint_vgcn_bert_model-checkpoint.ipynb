{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d9e183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam #, warmup_linear\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import *\n",
    "from model_vgcn_bert import VGCN_Bert\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f7655",
   "metadata": {},
   "source": [
    "#### Step 1:   Configurations for Evaluating VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ad4e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 1: CONFIGURATIONS FOR TRAINING--------\n",
      "Dataset:  pheme\n",
      "Will Load Model from Checkpoint:  True\n",
      "Will Delete Stop Words:  True\n",
      "Vocab GCN Hidden Dim: vocab_size -> 128 -> 16\n",
      "Learning Rate0:  1e-05\n",
      "Weight Decay:  0.01\n",
      "Loss Criterion:  cross_entropy\n",
      "Will Perform Softmax before MSE:  True\n",
      "Vocab Adjcent:  all\n",
      "MAX_SEQ_LENGTH:  216\n",
      "Perform Metrics:  ['weighted avg', 'f1-score']\n",
      "Saved Model File Name:  VGCN_BERT16_model_pheme_cross_entropy_sw1.pt\n"
     ]
    }
   ],
   "source": [
    "args = {\"ds\": \"pheme\", \"load\": 1, \"sw\": 1, \"dim\": 16,\n",
    "        \"lr\": 1e-5, \"l2\": 0.01, \"model\": \"VGCN_BERT\"}\n",
    "\n",
    "config_dataset = args[\"ds\"]\n",
    "config_load_model_from_checkpoint = True if args[\"load\"] == 1 else False\n",
    "config_use_stopwords = True if args[\"sw\"] == 1 else False\n",
    "config_gcn_embedding_dim = args[\"dim\"]\n",
    "config_learning_rate0 = args[\"lr\"]\n",
    "config_l2_decay = args[\"l2\"]\n",
    "config_model_type = args[\"model\"]\n",
    "\n",
    "config_warmup_proportion = 0.1\n",
    "config_vocab_adj = 'all'  # pmi / tf / all\n",
    "config_adj_npmi_threshold = 0.2\n",
    "config_adj_tf_threshold = 0\n",
    "config_loss_criterion = 'cross_entropy'\n",
    "\n",
    "MAX_SEQ_LENGTH = 200 + config_gcn_embedding_dim\n",
    "total_train_epochs = 9\n",
    "batch_size = 16  # 12\n",
    "gradient_accumulation_steps = 1\n",
    "bert_model_scale = 'bert-base-uncased'\n",
    "do_lower_case = True\n",
    "perform_metrics_str = ['weighted avg', 'f1-score']\n",
    "do_softmax_before_mse = True\n",
    "\n",
    "data_dir = './prepared_data/'\n",
    "output_dir = './model_output/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "model_file_save = config_model_type + str(config_gcn_embedding_dim) + '_model_' + \\\n",
    "    config_dataset + '_' + config_loss_criterion + '_' + \\\n",
    "    \"sw\" + str(int(config_use_stopwords)) + '.pt'\n",
    "\n",
    "print('----------STEP 1: CONFIGURATIONS FOR TRAINING--------')\n",
    "print('Dataset: ', config_dataset)\n",
    "print('Will Load Model from Checkpoint: ', config_load_model_from_checkpoint)\n",
    "print('Will Delete Stop Words: ', config_use_stopwords)\n",
    "print('Vocab GCN Hidden Dim: vocab_size -> 128 -> ' + str(config_gcn_embedding_dim))\n",
    "print('Learning Rate0: ', config_learning_rate0)\n",
    "print('Weight Decay: ', config_l2_decay)\n",
    "print('Loss Criterion: ', config_loss_criterion)\n",
    "print('Will Perform Softmax before MSE: ', do_softmax_before_mse)\n",
    "print('Vocab Adjcent: ', config_vocab_adj)\n",
    "print('MAX_SEQ_LENGTH: ', MAX_SEQ_LENGTH)\n",
    "print('Perform Metrics: ', perform_metrics_str)\n",
    "print('Saved Model File Name: ', model_file_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79eede6",
   "metadata": {},
   "source": [
    "#### Step 2.1: Prepare Dataset & Load Vocabulary Adjacent Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0e7e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------\n",
      " Load and seperate pheme dataset, with vocabulary graph adjacent matrix\n"
     ]
    }
   ],
   "source": [
    "print('----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------')\n",
    "print(' Load and seperate', config_dataset, 'dataset, with vocabulary graph adjacent matrix')\n",
    "\n",
    "objects = []\n",
    "names = ['index_label', 'train_label', 'train_label_prob', 'test_label',\n",
    "         'test_label_prob', 'clean_docs', 'vocab_adj_tf', 'vocab_adj_pmi', 'vocab_map']\n",
    "\n",
    "for i in range(len(names)):\n",
    "    datafile = data_dir + \"/data_%s.%s\" % (config_dataset, names[i])\n",
    "    with open(datafile, 'rb') as f:\n",
    "        objects.append(pkl.load(f, encoding='latin1'))\n",
    "\n",
    "index_labels_list, train_label, train_label_prob, test_label, test_label_prob, shuffled_clean_docs, gcn_vocab_adj_tf, gcn_vocab_adj_pmi, gcn_vocab_map = tuple(objects)\n",
    "\n",
    "label2idx = index_labels_list[0]\n",
    "idx2label = index_labels_list[1]\n",
    "\n",
    "all_labels = np.hstack((train_label, test_label))\n",
    "all_labels_prob = np.vstack((train_label_prob, test_label_prob))\n",
    "\n",
    "examples = []\n",
    "for i, text in enumerate(shuffled_clean_docs):\n",
    "    example = InputExample(i, text.strip(), confidence=all_labels_prob[i], label=all_labels[i])\n",
    "    examples.append(example)\n",
    "\n",
    "num_classes = len(label2idx)\n",
    "gcn_vocab_size = len(gcn_vocab_map)\n",
    "train_size = len(train_label)\n",
    "test_size = len(test_label)\n",
    "\n",
    "indexs = np.arange(0, len(examples))\n",
    "train_examples = [examples[i] for i in indexs[:train_size]]\n",
    "test_examples = [examples[i] for i in indexs[train_size:train_size + test_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa16c2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero ratio for vocab adj 0th: 77.09113559\n",
      "Zero ratio for vocab adj 1th: 94.97669180\n"
     ]
    }
   ],
   "source": [
    "if config_adj_tf_threshold > 0:\n",
    "    gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > config_adj_tf_threshold)\n",
    "    gcn_vocab_adj_tf.eliminate_zeros()\n",
    "if config_adj_npmi_threshold > 0:\n",
    "    gcn_vocab_adj_pmi.data *= (gcn_vocab_adj_pmi.data > config_adj_npmi_threshold)\n",
    "    gcn_vocab_adj_pmi.eliminate_zeros()\n",
    "\n",
    "if config_vocab_adj == 'pmi':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_pmi]\n",
    "elif config_vocab_adj == 'tf':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf]\n",
    "elif config_vocab_adj == 'all':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf, gcn_vocab_adj_pmi]\n",
    "\n",
    "norm_gcn_vocab_adj_list = []\n",
    "for i in range(len(gcn_vocab_adj_list)):\n",
    "    adj = gcn_vocab_adj_list[i]\n",
    "\n",
    "    print('Zero ratio for vocab adj %dth: %.8f' %\n",
    "          (i, 100 * (1 - adj.count_nonzero() / (adj.shape[0] * adj.shape[1]))))\n",
    "\n",
    "    adj = normalize_adj(adj)\n",
    "    norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
    "\n",
    "gcn_adj_list = norm_gcn_vocab_adj_list\n",
    "\n",
    "\n",
    "train_classes_num, train_classes_weight = get_class_count_and_weight(train_label, len(label2idx))\n",
    "loss_weight = torch.tensor(train_classes_weight).to(device)\n",
    "loss_weight = torch.tensor(loss_weight, dtype=torch.float32).to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac765aa4",
   "metadata": {},
   "source": [
    "#### Step 2.2:   Prepare PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "916e2c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classes Count:  [5341, 2368]\n",
      "Batch size:  16\n",
      "Num steps:  4338\n",
      "Number of Examples for Training:  7709\n",
      "Number of Examples for Training After Dataloader:  7712\n",
      "Number of Examples for Validate:  1647\n"
     ]
    }
   ],
   "source": [
    "def get_pytorch_dataloader(examples, tokenizer, batch_size):\n",
    "    dataset = CorpusDataset(examples, tokenizer, gcn_vocab_map, MAX_SEQ_LENGTH, config_gcn_embedding_dim)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=dataset.pad)\n",
    "\n",
    "\n",
    "train_dataloader = get_pytorch_dataloader(train_examples, tokenizer, batch_size)\n",
    "test_dataloader = get_pytorch_dataloader(test_examples, tokenizer, batch_size)\n",
    "\n",
    "total_train_steps = int(len(train_dataloader) / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print('Train Classes Count: ', train_classes_num)\n",
    "print('Batch size: ', batch_size)\n",
    "print('Num steps: ', total_train_steps)\n",
    "print('Number of Examples for Training: ', len(train_examples))\n",
    "print('Number of Examples for Training After Dataloader: ', len(train_dataloader) * batch_size)\n",
    "print('Number of Examples for Validate: ', len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df124ae",
   "metadata": {},
   "source": [
    "#### Step 3.1:   Load Trained VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf798ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the pretrain model: VGCN_BERT16_model_pheme_cross_entropy_sw1.pt , epoch: 7 step: -1 valid acc: 0.8955676988463873 weighted avg f1-score_valid: 0.895162552354317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGCN_Bert(\n",
       "  (embeddings): VGCNBertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (vocab_gcn): VocabGraphConvolution(\n",
       "      (fc_hc): Linear(in_features=128, out_features=16, bias=True)\n",
       "      (act_func): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config_load_model_from_checkpoint and os.path.exists(os.path.join(output_dir, model_file_save)):\n",
    "    checkpoint = torch.load(os.path.join(output_dir, model_file_save), map_location='cpu')\n",
    "    if 'step' in checkpoint:\n",
    "        prev_save_step = checkpoint['step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        prev_save_step = -1\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    valid_acc_prev = checkpoint['valid_acc']\n",
    "    perform_metrics_prev = checkpoint['perform_metrics']\n",
    "    model = VGCN_Bert.from_pretrained(bert_model_scale, state_dict=checkpoint['model_state'], gcn_adj_dim=gcn_vocab_size, \n",
    "        gcn_adj_num=len(gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "\n",
    "    pretrained_dict = checkpoint['model_state']\n",
    "    net_state_dict = model.state_dict()\n",
    "    pretrained_dict_selected = {\n",
    "        k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "    net_state_dict.update(pretrained_dict_selected)\n",
    "    model.load_state_dict(net_state_dict)\n",
    "\n",
    "    print('Loaded the pretrain model:', model_file_save, ', epoch:', checkpoint['epoch'], 'step:', prev_save_step, 'valid acc:',\n",
    "          checkpoint['valid_acc'], ' '.join(perform_metrics_str)+'_valid:', checkpoint['perform_metrics'])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579efee",
   "metadata": {},
   "source": [
    "#### Step 3.2: Evaluate VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77ef2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, gcn_adj_list,predict_dataloader, batch_size, dataset_name):\n",
    "    # print(\"***** Running prediction *****\")\n",
    "    model.eval()\n",
    "    predict_out = []\n",
    "    all_label_ids = []\n",
    "    ev_loss=0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(predict_dataloader, desc=\"Evaluating\", colour='green'):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            _, logits = model(gcn_adj_list, gcn_swop_eye,input_ids, segment_ids, input_mask)\n",
    "\n",
    "            if config_loss_criterion=='mse':\n",
    "                if do_softmax_before_mse:\n",
    "                    logits=F.softmax(logits,-1)\n",
    "                loss = F.mse_loss(logits, y_prob)\n",
    "            else:\n",
    "                if loss_weight is None:\n",
    "                    loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "                else:\n",
    "                    loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "                    \n",
    "            ev_loss+=loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits, -1)\n",
    "            \n",
    "            predict_out.extend(predicted.tolist())\n",
    "            all_label_ids.extend(label_ids.tolist())\n",
    "            eval_accuracy=predicted.eq(label_ids).sum().item()\n",
    "            total += len(label_ids)\n",
    "            correct += eval_accuracy\n",
    "\n",
    "        f1_metrics=f1_score(np.array(all_label_ids).reshape(-1),\n",
    "            np.array(predict_out).reshape(-1), average='weighted')\n",
    "        print(\"Report:\\n\"+classification_report(np.array(all_label_ids).reshape(-1),\n",
    "            np.array(predict_out).reshape(-1),digits=4))\n",
    "\n",
    "    ev_acc = correct/total\n",
    "    return ev_loss, ev_acc, f1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1343f224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|\u001b[32m████████████████████████\u001b[0m| 103/103 [03:42<00:00,  2.16s/it]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9161    0.9293    0.9227      1104\n",
      "           1     0.8520    0.8269    0.8393       543\n",
      "\n",
      "    accuracy                         0.8956      1647\n",
      "   macro avg     0.8840    0.8781    0.8810      1647\n",
      "weighted avg     0.8949    0.8956    0.8952      1647\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45.12523901253007, 0.8955676988463873, 0.895162552354317)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, gcn_adj_list, test_dataloader, batch_size, 'Test_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086eb13",
   "metadata": {},
   "source": [
    "#### Step 4.1: Get VGCN_BERT Model Pooled Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af635dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pooled_out(model, gcn_adj_list, predict_dataloader):\n",
    "    \n",
    "    outputs = None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(predict_dataloader, desc=\"Evaluating\", colour='green'):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            pooled_output, _= model(gcn_adj_list, gcn_swop_eye, input_ids, segment_ids, input_mask)\n",
    "            \n",
    "            if outputs is None:\n",
    "                outputs = pooled_output.detach().cpu().numpy()\n",
    "            else:\n",
    "                outputs = np.append(outputs, pooled_output.detach().cpu().numpy(), axis=0)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c0a1cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|\u001b[32m████████████████████████\u001b[0m| 482/482 [18:58<00:00,  2.36s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_pooled_outputs = get_pooled_out(model, gcn_adj_list, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4828bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|\u001b[32m████████████████████████\u001b[0m| 103/103 [04:16<00:00,  2.49s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_pooled_outputs = get_pooled_out(model, gcn_adj_list, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c95db355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7709, 768) (1647, 768)\n"
     ]
    }
   ],
   "source": [
    "print(train_pooled_outputs.shape, test_pooled_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c461f45",
   "metadata": {},
   "source": [
    "#### Step 4.2: Reorganize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30f1c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index_list = []\n",
    "with open('./data/PHEME-SEG/train_index_list.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        train_index_list.append(int(each_line))\n",
    "        \n",
    "test_index_list = []\n",
    "with open('./data/PHEME-SEG/test_index_list.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        test_index_list.append(int(each_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f6c4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = []\n",
    "with open('./data/PHEME-SEG/train_label.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        train_label.append(int(each_line))\n",
    "\n",
    "test_label = []\n",
    "with open('./data/PHEME-SEG/test_label.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        test_label.append(int(each_line))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e5549fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.32304156, -0.08786156, -0.9904806, -0.3332...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.9551921, 0.32555145, -0.4598627, -0.083611...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.13894607, -0.13198681, -0.44664404, 0.4097...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.34066188, 0.108433254, -0.8688264, -0.2292...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.2034768, -0.22863488, -0.6447237, -0.3498...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[-0.07788508, -0.11704441, -0.86137265, -0.35...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[0.2300627, 0.06864409, -0.87173355, -0.63888...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[-0.20682545, -0.56291294, -0.9688244, -0.184...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[-0.6474601, -0.3249243, 0.8925246, 0.7187841...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[-0.32293054, -0.23340979, -0.7573208, -0.321...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[0.32304156, -0.08786156, -0.9904806, -0.3332...      0\n",
       "1  [[0.9551921, 0.32555145, -0.4598627, -0.083611...      1\n",
       "2  [[0.13894607, -0.13198681, -0.44664404, 0.4097...      1\n",
       "3  [[0.34066188, 0.108433254, -0.8688264, -0.2292...      0\n",
       "4  [[-0.2034768, -0.22863488, -0.6447237, -0.3498...      0\n",
       "5  [[-0.07788508, -0.11704441, -0.86137265, -0.35...      0\n",
       "6  [[0.2300627, 0.06864409, -0.87173355, -0.63888...      0\n",
       "7  [[-0.20682545, -0.56291294, -0.9688244, -0.184...      0\n",
       "8  [[-0.6474601, -0.3249243, 0.8925246, 0.7187841...      1\n",
       "9  [[-0.32293054, -0.23340979, -0.7573208, -0.321...      0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = {}\n",
    "\n",
    "for l, emb in zip(train_index_list,train_pooled_outputs):\n",
    "    if l in train_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        train_x[l]  =np.vstack([train_x[l], emb])\n",
    "    else:\n",
    "        train_x[l] = [emb]\n",
    "\n",
    "train_l_final = []\n",
    "label_l_final = []\n",
    "\n",
    "for k in train_x.keys():\n",
    "    train_l_final.append(train_x[k])\n",
    "    label_l_final.append(train_label[k])\n",
    "\n",
    "df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9292e1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.24490148, -0.039963268, -0.24996834, 0.02...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.3585916, -0.20751837, 0.31686747, -0.1550...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.14235935, -0.13293198, -0.9014919, -0.345...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.16194391, -0.23561445, -0.8804464, -0.5395...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.23013046, -0.22067745, -0.96652675, -0.397...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[-0.035805054, -0.3766511, -0.94707274, -0.30...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[-0.16669868, -0.21498442, 0.8461548, 0.03320...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[-0.47793505, -0.21514018, -0.79592204, -0.12...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[-0.48226103, -0.55921304, -0.9144425, -0.070...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[-0.10675546, -0.4406971, -0.97483206, -0.284...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[-0.24490148, -0.039963268, -0.24996834, 0.02...      1\n",
       "1  [[-0.3585916, -0.20751837, 0.31686747, -0.1550...      0\n",
       "2  [[-0.14235935, -0.13293198, -0.9014919, -0.345...      0\n",
       "3  [[0.16194391, -0.23561445, -0.8804464, -0.5395...      0\n",
       "4  [[0.23013046, -0.22067745, -0.96652675, -0.397...      0\n",
       "5  [[-0.035805054, -0.3766511, -0.94707274, -0.30...      0\n",
       "6  [[-0.16669868, -0.21498442, 0.8461548, 0.03320...      1\n",
       "7  [[-0.47793505, -0.21514018, -0.79592204, -0.12...      0\n",
       "8  [[-0.48226103, -0.55921304, -0.9144425, -0.070...      0\n",
       "9  [[-0.10675546, -0.4406971, -0.97483206, -0.284...      0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = {}\n",
    "\n",
    "for l, emb in zip(test_index_list,test_pooled_outputs):\n",
    "    if l in test_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        test_x[l]  =np.vstack([test_x[l], emb])\n",
    "    else:\n",
    "        test_x[l] = [emb]\n",
    "\n",
    "test_l_final = []\n",
    "tlabel_l_final = []\n",
    "for k in test_x.keys():\n",
    "    test_l_final.append(test_x[k])\n",
    "    tlabel_l_final.append(test_label[k])\n",
    "\n",
    "df_test = pd.DataFrame({'emb': test_l_final, 'label': tlabel_l_final})\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8a3d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "811ef558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5221, 2) (331, 2) (332, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape,df_val.shape,df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c04c4",
   "metadata": {},
   "source": [
    "#### Step 4.3: Generate Data Generator for Joint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03cf37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = {\n",
    "    \"text_comments\": [[7,663], [3, 232], [5, 93]],\n",
    "    \"text_comments_refined\": [[23, 227], [1, 331], [4,83]],\n",
    "}\n",
    "\n",
    "batch = batch_dict[\"text_comments_refined\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "effcfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(df, batch_size, batches_per_epoch):\n",
    "    num_sequences = len(df['emb'].to_list())\n",
    "    assert batch_size * batches_per_epoch == num_sequences\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch):\n",
    "            longest_index = (b + 1) * batch_size - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size,  1))\n",
    "            for i in range(batch_size):\n",
    "                li = b * batch_size + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train\n",
    "            \n",
    "def val_generator(df,batch_size_val,batches_per_epoch_val):\n",
    "    num_sequences_val = len(df['emb'].to_list())\n",
    "    assert batch_size_val * batches_per_epoch_val == num_sequences_val\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_val):\n",
    "            longest_index = (b + 1) * batch_size_val - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_val][-31:], key=len))\n",
    "            x_val = np.full((batch_size_val, timesteps, num_features), -99.)\n",
    "            y_val = np.zeros((batch_size_val,  1))\n",
    "            for i in range(batch_size_val):\n",
    "                li = b * batch_size_val + i\n",
    "                x_val[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_val[i] = y_list[li]\n",
    "            yield x_val, y_val\n",
    "            \n",
    "def test_generator(df,batch_size_test, batches_per_epoch_test):\n",
    "    num_sequences_test = len(df['emb'].to_list())\n",
    "    assert batch_size_test * batches_per_epoch_test == num_sequences_test\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_test):\n",
    "            longest_index = (b + 1) * batch_size_test - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_test][-31:], key=len))\n",
    "            # print(len(df_train['emb'].to_list()[:b+batch_size][-7:]))\n",
    "            x_test = np.full((batch_size_test, timesteps, num_features), -99.)\n",
    "            y_test = np.zeros((batch_size_test,  1))\n",
    "            for i in range(batch_size_test):\n",
    "                li = b * batch_size_test + i\n",
    "                x_test[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_test[i] = y_list[li]\n",
    "            yield x_test, y_test            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "642aa512",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_generator(df_train, batch[0][0], batch[0][1])\n",
    "val_data = val_generator(df_val, batch[1][0], batch[1][1])\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af86ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def cul_all_metrics(y_true, y_pred, pos_label=1):\n",
    "    return {\"accuracy\": float(\"%.5f\" % accuracy_score(y_true=y_true, y_pred=y_pred)),\n",
    "            \"precision\": float(\"%.5f\" % precision_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label, average=\"weighted\")),\n",
    "            \"recall\": float(\"%.5f\" % recall_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label, average=\"weighted\")),\n",
    "            \"f1-score\": float(\"%.5f\" % f1_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")),\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b520a",
   "metadata": {},
   "source": [
    "#### Step 5: Build \"Recurrence over VGCN_BERT\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "716ee802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-19 04:54:58.949407: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-03-19 04:54:58.977180: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2099965000 Hz\n",
      "2022-03-19 04:54:58.980732: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5653480bca60 executing computations on platform Host. Devices:\n",
      "2022-03-19 04:54:58.980767: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "features (InputLayer)        [(None, None, 768)]       0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 350,692\n",
      "Trainable params: 350,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "\n",
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='features')\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "encoded_text = keras.layers.LSTM(100,)(l_mask)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_text)\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "R_Model = keras.Model(text_input, out)\n",
    "R_Model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "R_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4987216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84358d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 227 steps, validate for 331 steps\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-19 04:58:56.552298: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_5844_7301' and '__inference___backward_standard_lstm_7406_8003_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8125' both implement 'lstm_308c4334-d708-4111-a697-166322ce6f6a' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/227 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9936"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-19 04:59:03.845387: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_9377' and '__inference_standard_lstm_9036_specialized_for_model_lstm_StatefulPartitionedCall_at___inference_distributed_function_10907' both implement 'lstm_30e64ee1-d9ef-4380-9962-9da2c531a512' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 14s 63ms/step - loss: 0.0235 - acc: 0.9937 - val_loss: 0.4687 - val_acc: 0.9124\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 6s 28ms/step - loss: 0.0129 - acc: 0.9975 - val_loss: 0.4826 - val_acc: 0.9124\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.0121 - acc: 0.9975 - val_loss: 0.4799 - val_acc: 0.9124\n",
      "Epoch 4/10\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9977\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.0113 - acc: 0.9977 - val_loss: 0.4987 - val_acc: 0.9154\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.0112 - acc: 0.9977 - val_loss: 0.5188 - val_acc: 0.9154\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.0109 - acc: 0.9977 - val_loss: 0.5148 - val_acc: 0.9184\n",
      "Epoch 7/10\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9979\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.0108 - acc: 0.9979 - val_loss: 0.5150 - val_acc: 0.9154\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.0098 - acc: 0.9981 - val_loss: 0.5456 - val_acc: 0.9245\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.0096 - acc: 0.9981 - val_loss: 0.5510 - val_acc: 0.9215\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.0094 - acc: 0.9981 - val_loss: 0.5593 - val_acc: 0.9215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9283a63690>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = batch[0][1]\n",
    "\n",
    "batches_per_epoch_val= batch[1][1]\n",
    "\n",
    "R_Model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7edaee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batch[2][1]\n",
    "\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])\n",
    "r_score = R_Model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb268b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9006,\n",
       " 'precision': 0.90135,\n",
       " 'recall': 0.9006,\n",
       " 'f1-score': 0.9009}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_pred = np.argmax(r_score, axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label, r_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc669746",
   "metadata": {},
   "source": [
    "#### Step 6: Build \"Transformer over VGCN_BERT\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dbd7cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            embed_dim % num_heads == 0\n",
    "        ), \"embedding dimension not divisible by num heads\"\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.wq = keras.layers.Dense(embed_dim)\n",
    "        self.wk = keras.layers.Dense(embed_dim)\n",
    "        self.wv = keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dk)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, v)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        q = self.wq(x)  # (batch_size, seq_len, embed_dim)\n",
    "        k = self.wk(x)  # (batch_size, seq_len, embed_dim)\n",
    "        v = self.wv(x)  # (batch_size, seq_len, embed_dim)\n",
    "        q = self.separate_heads(\n",
    "            q, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        k = self.separate_heads(\n",
    "            k, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        v = self.separate_heads(\n",
    "            v, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(q, k, v)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.att(x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5dfb2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "features (InputLayer)        [(None, None, 768)]       0         \n",
      "_________________________________________________________________\n",
      "masking_2 (Masking)          (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 768)         2415392   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 2,766,084\n",
      "Trainable params: 2,766,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='features')\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "transformer_encodings = TransformerLayer(embed_dim=768, num_heads=1, ff_dim=32)(l_mask)\n",
    "encoded_texts = keras.layers.LSTM(100,)(transformer_encodings)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_texts)\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "T_Model = keras.Model(text_input, out)\n",
    "T_Model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "T_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21e1681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35496c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 227 steps, validate for 331 steps\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-20 04:09:36.757419: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_88269_88451' and '__inference___backward_standard_lstm_88556_89039_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_89617' both implement 'lstm_88a27c55-0707-4a1b-9178-a34034900660' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/227 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9911"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-20 04:09:47.369916: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_90762_specialized_for_model_1_lstm_1_StatefulPartitionedCall_at___inference_distributed_function_91128' and '__inference_cudnn_lstm_with_fallback_90873' both implement 'lstm_e3b4fd0e-500b-4841-91ea-c678a30d4d13' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 17s 76ms/step - loss: 0.0263 - acc: 0.9912 - val_loss: 0.5753 - val_acc: 0.9154\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 12s 51ms/step - loss: 0.0235 - acc: 0.9943 - val_loss: 0.4674 - val_acc: 0.9215\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 11s 51ms/step - loss: 0.0190 - acc: 0.9962 - val_loss: 0.4524 - val_acc: 0.9124\n",
      "Epoch 4/10\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9950\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "227/227 [==============================] - 11s 50ms/step - loss: 0.0190 - acc: 0.9950 - val_loss: 0.4399 - val_acc: 0.9124\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 11s 50ms/step - loss: 0.0180 - acc: 0.9952 - val_loss: 0.4840 - val_acc: 0.9184\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 11s 50ms/step - loss: 0.0164 - acc: 0.9971 - val_loss: 0.5040 - val_acc: 0.9184\n",
      "Epoch 7/10\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9967\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "227/227 [==============================] - 11s 49ms/step - loss: 0.0157 - acc: 0.9967 - val_loss: 0.4528 - val_acc: 0.9215\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 11s 49ms/step - loss: 0.0156 - acc: 0.9967 - val_loss: 0.4409 - val_acc: 0.9275\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 11s 49ms/step - loss: 0.0153 - acc: 0.9971 - val_loss: 0.4712 - val_acc: 0.9215\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 11s 50ms/step - loss: 0.0145 - acc: 0.9971 - val_loss: 0.4998 - val_acc: 0.9245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8fef0d1b50>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = batch[0][1]\n",
    "\n",
    "batches_per_epoch_val= batch[1][1]\n",
    "\n",
    "T_Model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca5768ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batch[2][1]\n",
    "\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])\n",
    "t_score = T_Model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6ef4707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.90964,\n",
       " 'precision': 0.91007,\n",
       " 'recall': 0.90964,\n",
       " 'f1-score': 0.90982}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_pred = np.argmax(t_score, axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label, t_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e026654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
