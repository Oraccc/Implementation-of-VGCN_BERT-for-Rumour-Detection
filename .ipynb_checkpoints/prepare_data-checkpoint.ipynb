{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7902b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl\n",
    "import re\n",
    "import time\n",
    "import argparse\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from turtle import window_width\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from get_data import DataReader\n",
    "from utils import *\n",
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8b078",
   "metadata": {},
   "source": [
    "#### Step 1:   Configs for Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feaabce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------STEP 1: CONFIGS FOR DATA PREPARE-----------\n",
      "Dataset:  pheme\n",
      "Min Frequency for Word Choice:  10\n",
      "Window Size:  1000\n",
      "Will Delete Stop Words:  1\n",
      "Will Use Bert Tokenizer at Clean:  True\n",
      "TF-IDF Mode:  all_tfidf\n",
      "Bert Model Scale:  bert-base-uncased\n",
      "Bert Lower Case:  True\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# # ds = dataset, sw = stopwords\n",
    "# parser.add_argument('--ds', type=str, default='pheme')\n",
    "# parser.add_argument('--sw', type=int, default=1)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = {\"ds\": \"pheme\", \"sw\": 1}\n",
    "# config_dataset = args.ds\n",
    "# config_use_stopwords = True if args.sw else False\n",
    "config_dataset = args[\"ds\"]\n",
    "config_use_stopwords = args[\"sw\"]\n",
    "\n",
    "\n",
    "dump_dir = './prepared_data'\n",
    "if not os.path.exists(dump_dir):\n",
    "    os.mkdir(dump_dir)\n",
    "\n",
    "if config_use_stopwords:\n",
    "    freq_min_for_word_choice = 10\n",
    "else:\n",
    "    freq_min_for_word_choice = 1\n",
    "\n",
    "\n",
    "window_size = 1000  # word co-occurence with context windows, use whole sentence\n",
    "\n",
    "tfidf_mode = 'all_tfidf'  # only_tf / all_tfidf\n",
    "\n",
    "\n",
    "use_tokenizer_at_clean_string = True\n",
    "\n",
    "bert_model_scale = 'bert-base-uncased' # bert-base-uncased / bert-large-uncased\n",
    "bert_lower_case = True\n",
    "\n",
    "print('-----------STEP 1: CONFIGS FOR DATA PREPARE-----------')\n",
    "print('Dataset: ', config_dataset)\n",
    "print('Min Frequency for Word Choice: ', freq_min_for_word_choice)\n",
    "print('Window Size: ', window_size)\n",
    "print('Will Delete Stop Words: ', config_use_stopwords,)\n",
    "print('Will Use Bert Tokenizer at Clean: ', use_tokenizer_at_clean_string)\n",
    "print('TF-IDF Mode: ', tfidf_mode)\n",
    "print('Bert Model Scale: ', bert_model_scale)\n",
    "print('Bert Lower Case: ', bert_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948b05e",
   "metadata": {},
   "source": [
    "#### Step 2.1:   Get Texts, Lables, Confidence from Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b1dde2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------STEP 2: GET TWEETS, LABELS, CONFIDENCE FROM DATA FILE-----------\n",
      "PHEME Dataset, train_szie: 7709, test_size: 1647\n"
     ]
    }
   ],
   "source": [
    "print('-----------STEP 2: GET TWEETS, LABELS, CONFIDENCE FROM DATA FILE-----------')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "train, test = DataReader(\n",
    "    \"data/PHEME-SEG/train.txt\", \"./data/PHEME-SEG/test.txt\").read()\n",
    "\n",
    "train_size = len(train)\n",
    "test_size = len(test)\n",
    "print('PHEME Dataset, train_szie: %d, test_size: %d' %\n",
    "      (train_size, test_size))\n",
    "\n",
    "trainset = {}\n",
    "testset = {}\n",
    "\n",
    "for data, dataset in [(train, trainset), (test, testset)]:\n",
    "    label = []\n",
    "    all_text = []\n",
    "    for line in data:\n",
    "        label.append(line[0])\n",
    "        all_text.append(line[1])\n",
    "    dataset[\"label\"] = label\n",
    "    dataset[\"data\"] = all_text\n",
    "\n",
    "label_to_index = {label: i for i, label in enumerate(testset['label'])}\n",
    "index_to_label = {i: label for i, label in enumerate(testset['label'])}\n",
    "\n",
    "corpus = trainset['data'] + testset['data']\n",
    "label = np.array(trainset['label'] + testset['label'])\n",
    "corpus_size = len(corpus)\n",
    "label_prob = np.eye(corpus_size, len(label_to_index))[label]\n",
    "\n",
    "doc_content_list = []\n",
    "for t in corpus:\n",
    "    doc_content_list.append(del_http_user_tokenize(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5803ecc",
   "metadata": {},
   "source": [
    "#### Step 2.2:   Get Statistics for Original Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e70bee17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for original text\n",
      "max_len: 200, max_len_id: 0, min_len: 3, min_len_id: 2321, avg_len: 155.30\n"
     ]
    }
   ],
   "source": [
    "max_len_seq = 0\n",
    "max_len_seq_idx = -1\n",
    "min_len_seq = 1000\n",
    "min_len_seq_idx = -1\n",
    "sen_len_list = []\n",
    "\n",
    "for i, seq in enumerate(doc_content_list):\n",
    "    seq = seq.split()\n",
    "    sen_len_list.append(len(seq))\n",
    "    if len(seq) < min_len_seq:\n",
    "        min_len_seq = len(seq)\n",
    "        min_len_seq_idx = i\n",
    "    if len(seq) > max_len_seq:\n",
    "        max_len_seq = len(seq)\n",
    "        max_len_seq_idx = i\n",
    "\n",
    "print('Statistics for original text')\n",
    "print('max_len: %d, max_len_id: %d, min_len: %d, min_len_id: %d, avg_len: %.2f'\n",
    "      % (max_len_seq, max_len_seq_idx, min_len_seq, min_len_seq_idx, np.array(sen_len_list).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f0648",
   "metadata": {},
   "source": [
    "#### Step 3.1:   Remove Stopwords from Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22b7aeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------STEP 3: TOKENIZE SENTENCES & REMOVE STOP WORDS FROM TEXTS-----------\n",
      "Stop words: {'a', 'most', 'too', \"haven't\", 'if', 'where', 'was', 'it', 'd', 'in', 'that', 'down', 'further', 'all', 'no', 'than', 'nor', 'same', 'can', 'been', 'these', 'the', \"she's\", 'just', 'ma', 'his', 'did', 'yours', 'why', 'mightn', 'once', 'of', 'when', \"needn't\", \"shouldn't\", 'myself', \"it's\", 'under', 'whom', \"that'll\", 'against', 'has', 'couldn', 'for', 'from', 'wouldn', 'll', \"you'll\", 'will', 'so', 'theirs', 'such', 'there', 'themselves', \"should've\", 'until', 'here', 'those', 'now', 'hadn', 'ourselves', \"you've\", \"weren't\", 'some', 'didn', 'while', \"couldn't\", 'wasn', 'other', 'had', 'ours', 'then', 'at', 'i', 'me', 'you', 'during', \"wasn't\", 'as', 'which', 'who', 'himself', 'am', 'were', 'after', 'its', 'o', 'yourselves', 'to', 'them', 'by', 'isn', 'up', 'we', 'through', 'haven', 'their', 'own', 'before', \"isn't\", 'only', \"mightn't\", 'should', \"won't\", 'below', \"don't\", 'but', 'your', 'about', 'they', 're', 'what', 'y', 'yourself', 'herself', 'few', 'very', 'm', \"hasn't\", \"hadn't\", \"mustn't\", 'or', 'over', 'won', 'her', 'each', 'my', 'are', 'having', 'does', 'both', 'hasn', \"wouldn't\", 'any', 'aren', 'he', 'she', 'being', 'with', 'don', 'an', 'into', 't', 'weren', 'itself', 'needn', 'hers', 'on', 'not', 'doing', 's', \"doesn't\", 'shan', 'above', 'our', \"aren't\", \"you're\", 'have', 'between', 'shouldn', 'this', 'and', 'ain', \"didn't\", 'doesn', 'off', 'be', \"you'd\", 'more', 've', 'out', \"shan't\", 'do', 'him', 'mustn', 'again', 'because', 'how', 'is'}\n",
      "Use bert_tokenizer for seperate words to bert vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenize Texts: 100%|\u001b[32m██████████████████████\u001b[0m| 9356/9356 [00:35<00:00, 262.90it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('-----------STEP 3: TOKENIZE SENTENCES & REMOVE STOP WORDS FROM TEXTS-----------')\n",
    "\n",
    "if config_use_stopwords:\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "#     nltk.download('stopwords')\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words = set(stop_words)\n",
    "else:\n",
    "    stop_words = {}\n",
    "print('Stop words:', stop_words)\n",
    "\n",
    "\n",
    "tmp_word_freq = {}  # to remove rare words\n",
    "new_doc_content_list = []\n",
    "\n",
    "# use bert_tokenizer for split the sentence\n",
    "if use_tokenizer_at_clean_string:\n",
    "    print('Use bert_tokenizer for seperate words to bert vocab')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "        bert_model_scale, do_lower_case=bert_lower_case)\n",
    "\n",
    "for doc_content in tqdm(doc_content_list, desc=\"Tokenize Texts\", colour='green'):\n",
    "    new_doc = clean_str(doc_content)\n",
    "\n",
    "    if use_tokenizer_at_clean_string:\n",
    "        sub_words = bert_tokenizer.tokenize(new_doc)\n",
    "        sub_doc = ' '.join(sub_words).strip()\n",
    "        new_doc = sub_doc\n",
    "    new_doc_content_list.append(new_doc)\n",
    "\n",
    "    for word in new_doc.split():\n",
    "        if word in tmp_word_freq:\n",
    "            tmp_word_freq[word] += 1\n",
    "        else:\n",
    "            tmp_word_freq[word] = 1\n",
    "\n",
    "doc_content_list = new_doc_content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0b4cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 0  docs are empty.\n"
     ]
    }
   ],
   "source": [
    "clean_docs = []\n",
    "count_void_doc = 0\n",
    "\n",
    "for i, doc_content in enumerate(doc_content_list):\n",
    "    words = doc_content.split()\n",
    "    doc_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word not in stop_words and tmp_word_freq[word] >= freq_min_for_word_choice:\n",
    "            doc_words.append(word)\n",
    "\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "\n",
    "    if doc_str == '':\n",
    "        count_void_doc += 1\n",
    "        print('No.', i, 'is a empty doc after treat, replaced by \\'%s\\'. original: %s' % (\n",
    "            doc_str, doc_content))\n",
    "    clean_docs.append(doc_str)\n",
    "\n",
    "\n",
    "print('Total', count_void_doc, ' docs are empty.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0c8ec",
   "metadata": {},
   "source": [
    "#### Step 3.2:   Get Statistics for Spilt and Cleaned Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9c13918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics after stopwords and tokenizer:\n",
      "min_len : 2, min_len_id: 7918\n",
      "max_len : 459, max_len_id: 7017\n",
      "average_len : 121.32364258230012\n"
     ]
    }
   ],
   "source": [
    "min_len = 10000\n",
    "min_len_id = -1\n",
    "max_len = 0\n",
    "max_len_id = -1\n",
    "aver_len = 0\n",
    "\n",
    "for i, line in enumerate(clean_docs):\n",
    "    temp = line.strip().split()\n",
    "    aver_len = aver_len + len(temp)\n",
    "    if len(temp) < min_len:\n",
    "        min_len = len(temp)\n",
    "        min_len_id = i\n",
    "    if len(temp) > max_len:\n",
    "        max_len = len(temp)\n",
    "        max_len_id = i\n",
    "\n",
    "aver_len = 1.0 * aver_len / len(clean_docs)\n",
    "print('Statistics after stopwords and tokenizer:')\n",
    "print('min_len : ' + str(min_len) + ', min_len_id: ' + str(min_len_id))\n",
    "print('max_len : ' + str(max_len) + ', max_len_id: ' + str(max_len_id))\n",
    "print('average_len : ' + str(aver_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3ea9e",
   "metadata": {},
   "source": [
    "#### Step 4.1:   Prepare Data for Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c778faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_docs = clean_docs[: train_size]\n",
    "# test_docs = clean_docs[train_size : train_size + test_size]\n",
    "\n",
    "train_label = label[: train_size]\n",
    "test_label = label[train_size: train_size + test_size]\n",
    "\n",
    "train_label_prob = label_prob[: train_size]\n",
    "test_label_prob = label_prob[train_size: train_size + test_size]\n",
    "\n",
    "# build vocab using whole corpus(train + test + genelization)\n",
    "word_set = set()\n",
    "for doc_words in clean_docs:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_map = {}\n",
    "for i in range(vocab_size):\n",
    "    vocab_map[vocab[i]] = i\n",
    "\n",
    "word_doc_list = {}\n",
    "for i in range(len(clean_docs)):\n",
    "    doc_words = clean_docs[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8ecf3",
   "metadata": {},
   "source": [
    "#### Step 4.2:   Build Document-Word Heterogeneous Graph and Vocabulary Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0db704f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 4: BUILD GRAPH----------\n",
      "Calculate first isomerous adj and first isomorphic vocab adj, get word-word PMI values\n",
      "Train_valid size: 9356 , Window number: 9356\n"
     ]
    }
   ],
   "source": [
    "print('----------STEP 4: BUILD GRAPH----------')\n",
    "\n",
    "print('Calculate first isomerous adj and first isomorphic vocab adj, get word-word PMI values')\n",
    "\n",
    "adj_label = np.hstack((train_label, np.zeros(vocab_size), test_label))\n",
    "adj_label_prob = np.vstack((train_label_prob, np.zeros((vocab_size, len(\n",
    "    label_to_index)), dtype=np.float32), test_label_prob))\n",
    "\n",
    "windows = []\n",
    "for doc_words in clean_docs:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "\n",
    "print('Train_valid size:', len(clean_docs), ', Window number:', len(windows))\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "652ca074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word Coccurence within Windows: 100%|\u001b[32m███████\u001b[0m| 9356/9356 [03:41<00:00, 42.18it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "word_pair_count = {}\n",
    "\n",
    "for window in tqdm(windows, desc=\"Word Cooccurence within Windows\", colour='green'):\n",
    "    appeared = set()\n",
    "\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = vocab_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = vocab_map[word_j]\n",
    "\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in appeared:\n",
    "                continue\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            appeared.add(word_pair_str)\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in appeared:\n",
    "                continue\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            appeared.add(word_pair_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0569539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting PMI and NPMI: 100%|\u001b[32m████\u001b[0m| 15968108/15968108 [01:25<00:00, 187026.49it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_pmi: 8.04516083903596 min_pmi: -4.963507385920485\n",
      "max_npmi: 1.0 min_npmi: -0.5527229303170397\n"
     ]
    }
   ],
   "source": [
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "tfidf_row = []\n",
    "tfidf_col = []\n",
    "tfidf_weight = []\n",
    "vocab_adj_row = []\n",
    "vocab_adj_col = []\n",
    "vocab_adj_weight = []\n",
    "\n",
    "num_window = len(windows)\n",
    "tmp_max_npmi = 0\n",
    "tmp_min_npmi = 0\n",
    "tmp_max_pmi = 0\n",
    "tmp_min_pmi = 0\n",
    "\n",
    "for key in tqdm(word_pair_count, desc=\"Counting PMI and NPMI: \", colour='green'):\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j / (num_window * num_window)))\n",
    "\n",
    "    npmi = log(1.0 * word_freq_i * word_freq_j / (num_window *\n",
    "               num_window)) / log(1.0 * count / num_window) - 1\n",
    "\n",
    "    if npmi > tmp_max_npmi:\n",
    "        tmp_max_npmi = npmi\n",
    "    if npmi < tmp_min_npmi:\n",
    "        tmp_min_npmi = npmi\n",
    "    if pmi > tmp_max_pmi:\n",
    "        tmp_max_pmi = pmi\n",
    "    if pmi < tmp_min_pmi:\n",
    "        tmp_min_pmi = pmi\n",
    "    if pmi > 0:\n",
    "        row.append(train_size + i)\n",
    "        col.append(train_size + j)\n",
    "        weight.append(pmi)\n",
    "    if npmi > 0:\n",
    "        vocab_adj_row.append(i)\n",
    "        vocab_adj_col.append(j)\n",
    "        vocab_adj_weight.append(npmi)\n",
    "\n",
    "print('max_pmi:', tmp_max_pmi, 'min_pmi:', tmp_min_pmi)\n",
    "print('max_npmi:', tmp_max_npmi, 'min_npmi:', tmp_min_npmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4d113",
   "metadata": {},
   "source": [
    "#### Step 5.1:   Calculate Document-Word TF-IDF Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a8ffcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = len(clean_docs)\n",
    "doc_word_freq = {}\n",
    "for doc_id in range(n_docs):\n",
    "    doc_words = clean_docs[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = vocab_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(n_docs):\n",
    "    doc_words = clean_docs[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    tfidf_vec = []\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = vocab_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        tf = doc_word_freq[key]\n",
    "        tfidf_row.append(i)\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        tfidf_col.append(j)\n",
    "        col.append(train_size + j)\n",
    "        # smooth\n",
    "        idf = log((1.0 + n_docs) / (1.0+word_doc_freq[vocab[j]])) + 1.0\n",
    "        # weight.append(tf * idf)\n",
    "        if tfidf_mode == 'only_tf':\n",
    "            tfidf_vec.append(tf)\n",
    "        else:\n",
    "            tfidf_vec.append(tf * idf)\n",
    "        doc_word_set.add(word)\n",
    "    if len(tfidf_vec) > 0:\n",
    "        weight.extend(tfidf_vec)\n",
    "        tfidf_weight.extend(tfidf_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a80423",
   "metadata": {},
   "source": [
    "#### Step 5.2:   Assemble Adjacency Matrix and Dump to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79599e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 5: ASSEMBLE ADJACENCY MATRIX AND DUMP TO FILES----------\n",
      "Calculate isomorphic vocab adjacency matrix using doc's tf-idf...\n",
      "Check adjacent matrix...\n"
     ]
    }
   ],
   "source": [
    "print('----------STEP 5: ASSEMBLE ADJACENCY MATRIX AND DUMP TO FILES----------')\n",
    "\n",
    "node_size = vocab_size + corpus_size\n",
    "\n",
    "adj_list = []\n",
    "adj_list.append(sp.csr_matrix((weight, (row, col)),\n",
    "                shape=(node_size, node_size), dtype=np.float32))\n",
    "for i, adj in enumerate(adj_list):\n",
    "    adj_list[i] = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj_list[i].setdiag(1.0)\n",
    "\n",
    "vocab_adj = sp.csr_matrix((vocab_adj_weight, (vocab_adj_row, vocab_adj_col)), shape=(\n",
    "    vocab_size, vocab_size), dtype=np.float32)\n",
    "vocab_adj.setdiag(1.0)\n",
    "\n",
    "print('Calculate isomorphic vocab adjacency matrix using doc\\'s tf-idf...')\n",
    "tfidf_all = sp.csr_matrix((tfidf_weight, (tfidf_row, tfidf_col)), shape=(\n",
    "    corpus_size, vocab_size), dtype=np.float32)\n",
    "tfidf_train = tfidf_all[:train_size]\n",
    "tfidf_test = tfidf_all[train_size:train_size + test_size]\n",
    "\n",
    "tfidf_X_list = [tfidf_train, tfidf_test]\n",
    "vocab_tfidf = tfidf_all.T.tolil()\n",
    "for i in range(vocab_size):\n",
    "    norm = np.linalg.norm(vocab_tfidf.data[i])\n",
    "    if norm > 0:\n",
    "        vocab_tfidf.data[i] /= norm\n",
    "vocab_adj_tf = vocab_tfidf.dot(vocab_tfidf.T)\n",
    "\n",
    "# check\n",
    "print('Check adjacent matrix...')\n",
    "for k in range(len(adj_list)):\n",
    "    count = 0\n",
    "    for i in range(adj_list[k].shape[0]):\n",
    "        if adj_list[k][i, i] <= 0:\n",
    "            count += 1\n",
    "            print('No.%d adj, abnormal diagonal found, No.%d' % (k, i))\n",
    "    if count > 0:\n",
    "        print('No.%d adj, total %d zero diagonal found.' % (k, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "289e6610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump objects...\n",
      "Data prepared, spend 398.80 s\n"
     ]
    }
   ],
   "source": [
    "print('Dump objects...')\n",
    "\n",
    "with open(dump_dir+\"/data_%s.index_label\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump([label_to_index, index_to_label], f)\n",
    "\n",
    "with open(dump_dir+\"/data_%s.vocab_map\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(vocab_map, f)\n",
    "\n",
    "with open(dump_dir+\"/data_%s.vocab\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(vocab, f)\n",
    "\n",
    "with open(dump_dir+\"/data_%s.adj_list\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(adj_list, f)\n",
    "with open(dump_dir+\"/data_%s.label\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(label, f)\n",
    "with open(dump_dir+\"/data_%s.label_prob\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(label_prob, f)\n",
    "with open(dump_dir+\"/data_%s.train_label\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(train_label, f)\n",
    "with open(dump_dir+\"/data_%s.train_label_prob\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(train_label_prob, f)\n",
    "with open(dump_dir+\"/data_%s.test_label\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(test_label, f)\n",
    "with open(dump_dir+\"/data_%s.test_label_prob\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(test_label_prob, f)\n",
    "with open(dump_dir+\"/data_%s.tfidf_list\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(tfidf_X_list, f)\n",
    "with open(dump_dir+\"/data_%s.vocab_adj_pmi\" % (config_dataset), 'wb') as f:\n",
    "    pkl.dump(vocab_adj, f)\n",
    "with open(dump_dir+\"/data_%s.vocab_adj_tf\" % (config_dataset), 'wb') as f:\n",
    "    pkl.dump(vocab_adj_tf, f)\n",
    "with open(dump_dir+\"/data_%s.clean_docs\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(clean_docs, f)\n",
    "\n",
    "print('Data prepared, spend %.2f s' % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647fe00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
