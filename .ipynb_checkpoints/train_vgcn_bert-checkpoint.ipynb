{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f325a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam  # , warmup_linear\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from model_vgcn_bert import VGCN_Bert\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9c6f6",
   "metadata": {},
   "source": [
    "#### Step 1:   Configurations for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d47f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 1: CONFIGURATIONS FOR TRAINING--------\n",
      "Dataset:  pheme\n",
      "Will Load Model from Checkpoint:  False\n",
      "Will Delete Stop Words:  True\n",
      "Vocab GCN Hidden Dim: vocab_size -> 128 -> 16\n",
      "Learning Rate0:  1e-05\n",
      "Weight Decay:  0.01\n",
      "Loss Criterion:  cross_entropy\n",
      "Will Perform Softmax before MSE:  True\n",
      "Vocab Adjcent:  all\n",
      "MAX_SEQ_LENGTH:  216\n",
      "Perform Metrics:  ['weighted avg', 'f1-score']\n",
      "Saved Model File Name:  VGCN_BERT16_model_pheme_cross_entropy_sw1.pt\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# # ds = dataset, sw = stopwords, lr = learning rate, l2 = L2 regularization\n",
    "# parser.add_argument('--ds', type=str, default='pheme')\n",
    "# parser.add_argument('--load', type=int, default='0')\n",
    "# parser.add_argument('--sw', type=int, default='1')\n",
    "# parser.add_argument('--dim', type=int, default='16')\n",
    "# parser.add_argument('--lr', type=float, default=1e-5)  # 2e-5\n",
    "# parser.add_argument('--l2', type=float, default=0.01)  # 0.001\n",
    "# parser.add_argument('--model', type=str, default='VGCN_BERT')\n",
    "# args = parser.parse_args()\n",
    "args = {\"ds\": \"pheme\", \"load\": 0, \"sw\": 1, \"dim\": 16,\n",
    "        \"lr\": 1e-5, \"l2\": 0.01, \"model\": \"VGCN_BERT\"}\n",
    "\n",
    "# config_dataset = args.ds\n",
    "# config_load_model_from_checkpoint = True if args.load == 1 else False\n",
    "# config_use_stopwords = True if args.sw == 1 else False\n",
    "# config_gcn_embedding_dim = args.dim\n",
    "# config_learning_rate0 = args.lr\n",
    "# config_l2_decay = args.l2\n",
    "# config_model_type = args.model\n",
    "\n",
    "config_dataset = args[\"ds\"]\n",
    "config_load_model_from_checkpoint = True if args[\"load\"] == 1 else False\n",
    "config_use_stopwords = True if args[\"sw\"] == 1 else False\n",
    "config_gcn_embedding_dim = args[\"dim\"]\n",
    "config_learning_rate0 = args[\"lr\"]\n",
    "config_l2_decay = args[\"l2\"]\n",
    "config_model_type = args[\"model\"]\n",
    "\n",
    "config_warmup_proportion = 0.1\n",
    "config_vocab_adj = 'all'  # pmi / tf / all\n",
    "config_adj_npmi_threshold = 0.2\n",
    "config_adj_tf_threshold = 0\n",
    "config_loss_criterion = 'cross_entropy'\n",
    "\n",
    "MAX_SEQ_LENGTH = 200 + config_gcn_embedding_dim\n",
    "total_train_epochs = 9\n",
    "batch_size = 16  # 12\n",
    "gradient_accumulation_steps = 1\n",
    "bert_model_scale = 'bert-base-uncased'\n",
    "do_lower_case = True\n",
    "perform_metrics_str = ['weighted avg', 'f1-score']\n",
    "do_softmax_before_mse = True\n",
    "\n",
    "data_dir = './prepared_data/'\n",
    "output_dir = './model_output/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "model_file_save = config_model_type + str(config_gcn_embedding_dim) + '_model_' + \\\n",
    "    config_dataset + '_' + config_loss_criterion + '_' + \\\n",
    "    \"sw\" + str(int(config_use_stopwords)) + '.pt'\n",
    "\n",
    "print('----------STEP 1: CONFIGURATIONS FOR TRAINING--------')\n",
    "print('Dataset: ', config_dataset)\n",
    "print('Will Load Model from Checkpoint: ', config_load_model_from_checkpoint)\n",
    "print('Will Delete Stop Words: ', config_use_stopwords)\n",
    "print('Vocab GCN Hidden Dim: vocab_size -> 128 -> ' + str(config_gcn_embedding_dim))\n",
    "print('Learning Rate0: ', config_learning_rate0)\n",
    "print('Weight Decay: ', config_l2_decay)\n",
    "print('Loss Criterion: ', config_loss_criterion)\n",
    "print('Will Perform Softmax before MSE: ', do_softmax_before_mse)\n",
    "print('Vocab Adjcent: ', config_vocab_adj)\n",
    "print('MAX_SEQ_LENGTH: ', MAX_SEQ_LENGTH)\n",
    "print('Perform Metrics: ', perform_metrics_str)\n",
    "print('Saved Model File Name: ', model_file_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fab5ac",
   "metadata": {},
   "source": [
    "#### Step 2.1: Prepare Dataset & Load Vocabulary Adjacent Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc1a634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------\n",
      " Load and seperate pheme dataset, with vocabulary graph adjacent matrix\n"
     ]
    }
   ],
   "source": [
    "print('----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------')\n",
    "print(' Load and seperate', config_dataset, 'dataset, with vocabulary graph adjacent matrix')\n",
    "\n",
    "objects = []\n",
    "names = ['index_label', 'train_label', 'train_label_prob', 'test_label',\n",
    "         'test_label_prob', 'clean_docs', 'vocab_adj_tf', 'vocab_adj_pmi', 'vocab_map']\n",
    "\n",
    "for i in range(len(names)):\n",
    "    datafile = data_dir + \"/data_%s.%s\" % (config_dataset, names[i])\n",
    "    with open(datafile, 'rb') as f:\n",
    "        objects.append(pkl.load(f, encoding='latin1'))\n",
    "\n",
    "index_labels_list, train_label, train_label_prob, test_label, test_label_prob, shuffled_clean_docs, gcn_vocab_adj_tf, gcn_vocab_adj_pmi, gcn_vocab_map = tuple(objects)\n",
    "\n",
    "label2idx = index_labels_list[0]\n",
    "idx2label = index_labels_list[1]\n",
    "\n",
    "all_labels = np.hstack((train_label, test_label))\n",
    "all_labels_prob = np.vstack((train_label_prob, test_label_prob))\n",
    "\n",
    "examples = []\n",
    "for i, text in enumerate(shuffled_clean_docs):\n",
    "    example = InputExample(i, text.strip(), confidence=all_labels_prob[i], label=all_labels[i])\n",
    "    examples.append(example)\n",
    "\n",
    "num_classes = len(label2idx)\n",
    "gcn_vocab_size = len(gcn_vocab_map)\n",
    "train_size = len(train_label)\n",
    "test_size = len(test_label)\n",
    "\n",
    "indexs = np.arange(0, len(examples))\n",
    "train_examples = [examples[i] for i in indexs[:train_size]]\n",
    "test_examples = [examples[i] for i in indexs[train_size:train_size + test_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363fd7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero ratio for vocab adj 0th: 79.40479542\n",
      "Zero ratio for vocab adj 1th: 95.22829647\n"
     ]
    }
   ],
   "source": [
    "if config_adj_tf_threshold > 0:\n",
    "    gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > config_adj_tf_threshold)\n",
    "    gcn_vocab_adj_tf.eliminate_zeros()\n",
    "if config_adj_npmi_threshold > 0:\n",
    "    gcn_vocab_adj_pmi.data *= (gcn_vocab_adj_pmi.data > config_adj_npmi_threshold)\n",
    "    gcn_vocab_adj_pmi.eliminate_zeros()\n",
    "\n",
    "if config_vocab_adj == 'pmi':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_pmi]\n",
    "elif config_vocab_adj == 'tf':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf]\n",
    "elif config_vocab_adj == 'all':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf, gcn_vocab_adj_pmi]\n",
    "\n",
    "norm_gcn_vocab_adj_list = []\n",
    "for i in range(len(gcn_vocab_adj_list)):\n",
    "    adj = gcn_vocab_adj_list[i]\n",
    "\n",
    "    print('Zero ratio for vocab adj %dth: %.8f' %\n",
    "          (i, 100 * (1 - adj.count_nonzero() / (adj.shape[0] * adj.shape[1]))))\n",
    "\n",
    "    adj = normalize_adj(adj)\n",
    "    norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
    "\n",
    "gcn_adj_list = norm_gcn_vocab_adj_list\n",
    "\n",
    "\n",
    "train_classes_num, train_classes_weight = get_class_count_and_weight(train_label, len(label2idx))\n",
    "loss_weight = torch.tensor(train_classes_weight).to(device)\n",
    "loss_weight = torch.tensor(loss_weight, dtype=torch.float32).to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e606fcc",
   "metadata": {},
   "source": [
    "#### Step 2.2:   Prepare PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed4db8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classes Count:  [5341, 2368]\n",
      "Batch size:  16\n",
      "Num steps:  4338\n",
      "Number of Examples for Training:  7709\n",
      "Number of Examples for Training After Dataloader:  7712\n",
      "Number of Examples for Validate:  3174\n"
     ]
    }
   ],
   "source": [
    "def get_pytorch_dataloader(examples, tokenizer, batch_size):\n",
    "    dataset = CorpusDataset(examples, tokenizer, gcn_vocab_map, MAX_SEQ_LENGTH, config_gcn_embedding_dim)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=dataset.pad)\n",
    "\n",
    "\n",
    "train_dataloader = get_pytorch_dataloader(train_examples, tokenizer, batch_size)\n",
    "test_dataloader = get_pytorch_dataloader(test_examples, tokenizer, batch_size)\n",
    "\n",
    "total_train_steps = int(len(train_dataloader) / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print('Train Classes Count: ', train_classes_num)\n",
    "print('Batch size: ', batch_size)\n",
    "print('Num steps: ', total_train_steps)\n",
    "print('Number of Examples for Training: ', len(train_examples))\n",
    "print('Number of Examples for Training After Dataloader: ', len(train_dataloader) * batch_size)\n",
    "print('Number of Examples for Validate: ', len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d32200",
   "metadata": {},
   "source": [
    "#### Step 3.1:  Define Evaluating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc8f26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, examples, tokenizer, batch_size):\n",
    "    dataloader = get_pytorch_dataloader(examples, tokenizer, batch_size)\n",
    "    predict_out = []\n",
    "    confidence_out = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, _, _, gcn_swop_eye = batch\n",
    "\n",
    "            _, score_out = model(gcn_adj_list, gcn_swop_eye,\n",
    "                                 input_ids, segment_ids, input_mask)\n",
    "            if config_loss_criterion == 'mse' and do_softmax_before_mse:\n",
    "                score_out = torch.nn.functional.softmax(score_out, dim=-1)\n",
    "            predict_out.extend(score_out.max(1)[1].tolist())\n",
    "            confidence_out.extend(score_out.max(1)[0].tolist())\n",
    "\n",
    "    return np.array(predict_out).reshape(-1), np.array(confidence_out).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865ad863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, gcn_adj_list, predict_dataloader, epoch_th, dataset_name):\n",
    "    model.eval()\n",
    "    predict_out = []\n",
    "    all_label_ids = []\n",
    "    ev_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in predict_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            _, logits = model(gcn_adj_list, gcn_swop_eye,input_ids,  segment_ids, input_mask)\n",
    "\n",
    "            if config_loss_criterion == 'mse':\n",
    "                if do_softmax_before_mse:\n",
    "                    logits = F.softmax(logits, -1)\n",
    "                loss = F.mse_loss(logits, y_prob)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "            ev_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, -1)\n",
    "            predict_out.extend(predicted.tolist())\n",
    "            all_label_ids.extend(label_ids.tolist())\n",
    "            eval_accuracy = predicted.eq(label_ids).sum().item()\n",
    "            total += len(label_ids)\n",
    "            correct += eval_accuracy\n",
    "\n",
    "        f1_metrics = f1_score(np.array(all_label_ids).reshape(-1), np.array(predict_out).reshape(-1), average='weighted')\n",
    "        print(\"Report:\\n\" + classification_report(np.array(all_label_ids).reshape(-1), np.array(predict_out).reshape(-1), digits=4))\n",
    "\n",
    "    ev_acc = correct/total\n",
    "    end = time.time()\n",
    "    print('Epoch : %d, %s: %.3f Acc : %.3f on %s, Spend:%.3f minutes for evaluation'\n",
    "          % (epoch_th, ' '.join(perform_metrics_str), 100 * f1_metrics, 100. * ev_acc, dataset_name, (end - start) / 60.0))\n",
    "    print('*' * 50)\n",
    "    return ev_loss, ev_acc, f1_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0f581",
   "metadata": {},
   "source": [
    "#### Step 3.2:   Load / Initialize VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77bd8c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 3: START TRAINING VGCN_BERT MODEL----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGCN_Bert(\n",
       "  (embeddings): VGCNBertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (vocab_gcn): VocabGraphConvolution(\n",
       "      (fc_hc): Linear(in_features=128, out_features=16, bias=True)\n",
       "      (act_func): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----------STEP 3: START TRAINING VGCN_BERT MODEL----------')\n",
    "\n",
    "if config_load_model_from_checkpoint and os.path.exists(os.path.join(output_dir, model_file_save)):\n",
    "    checkpoint = torch.load(os.path.join(output_dir, model_file_save), map_location='cpu')\n",
    "    if 'step' in checkpoint:\n",
    "        prev_save_step = checkpoint['step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        prev_save_step = -1\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    valid_acc_prev = checkpoint['valid_acc']\n",
    "    perform_metrics_prev = checkpoint['perform_metrics']\n",
    "    model = VGCN_Bert.from_pretrained(bert_model_scale, state_dict=checkpoint['model_state'], gcn_adj_dim=gcn_vocab_size, \n",
    "        gcn_adj_num=len(gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "\n",
    "    pretrained_dict = checkpoint['model_state']\n",
    "    net_state_dict = model.state_dict()\n",
    "    pretrained_dict_selected = {\n",
    "        k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "    net_state_dict.update(pretrained_dict_selected)\n",
    "    model.load_state_dict(net_state_dict)\n",
    "\n",
    "    print('Loaded the pretrain model:', model_file_save, ', epoch:', checkpoint['epoch'], 'step:', prev_save_step, 'valid acc:',\n",
    "          checkpoint['valid_acc'], ' '.join(perform_metrics_str) + '_valid:', checkpoint['perform_metrics'])\n",
    "\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    valid_acc_prev = 0\n",
    "    perform_metrics_prev = 0\n",
    "    model = VGCN_Bert.from_pretrained(bert_model_scale, gcn_adj_dim=gcn_vocab_size, gcn_adj_num=len(\n",
    "        gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "    prev_save_step = -1\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f99d6",
   "metadata": {},
   "source": [
    "#### Step 3.3:   Start Training VGCN_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "040c2068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0-0/482, Train cross_entropy Loss: 0.7049648761749268, Cumulated Time: 0.03529953161875407m \n",
      "Epoch:0-40/482, Train cross_entropy Loss: 0.6887837052345276, Cumulated Time: 0.9651971499125163m \n",
      "Epoch:0-80/482, Train cross_entropy Loss: 0.9078288674354553, Cumulated Time: 2.9012184023857115m \n",
      "Epoch:0-120/482, Train cross_entropy Loss: 0.7365488409996033, Cumulated Time: 6.57899470726649m \n",
      "Epoch:0-160/482, Train cross_entropy Loss: 0.4911724030971527, Cumulated Time: 10.655984644095103m \n",
      "Epoch:0-200/482, Train cross_entropy Loss: 0.38617825508117676, Cumulated Time: 14.785737041632334m \n",
      "Epoch:0-240/482, Train cross_entropy Loss: 0.5448022484779358, Cumulated Time: 18.907255486647287m \n",
      "Epoch:0-280/482, Train cross_entropy Loss: 0.6284916996955872, Cumulated Time: 23.05172154108683m \n",
      "Epoch:0-320/482, Train cross_entropy Loss: 0.23829129338264465, Cumulated Time: 27.3492790778478m \n",
      "Epoch:0-360/482, Train cross_entropy Loss: 0.646953821182251, Cumulated Time: 31.709779862562815m \n",
      "Epoch:0-400/482, Train cross_entropy Loss: 0.4879439175128937, Cumulated Time: 35.96413789192835m \n",
      "Epoch:0-440/482, Train cross_entropy Loss: 0.2659659683704376, Cumulated Time: 40.13768372138341m \n",
      "Epoch:0-480/482, Train cross_entropy Loss: 0.23698413372039795, Cumulated Time: 44.459847537676495m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8604    0.8814    0.8708      2167\n",
      "           1     0.7306    0.6922    0.7109      1007\n",
      "\n",
      "    accuracy                         0.8214      3174\n",
      "   macro avg     0.7955    0.7868    0.7908      3174\n",
      "weighted avg     0.8192    0.8214    0.8200      3174\n",
      "\n",
      "Epoch : 0, weighted avg f1-score: 82.003 Acc : 82.136 on Test_set, Spend:8.117 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:0 Completed, Total Train Loss:273.16755218058825, Test Loss:83.01411652565002, Spend 52.67204522689183m \n",
      "Epoch:1-0/482, Train cross_entropy Loss: 0.8928995132446289, Cumulated Time: 52.84928981463114m \n",
      "Epoch:1-40/482, Train cross_entropy Loss: 0.39498236775398254, Cumulated Time: 57.08852729399999m \n",
      "Epoch:1-80/482, Train cross_entropy Loss: 0.5055928826332092, Cumulated Time: 61.44788438479106m \n",
      "Epoch:1-120/482, Train cross_entropy Loss: 0.2586160898208618, Cumulated Time: 65.82373105287552m \n",
      "Epoch:1-160/482, Train cross_entropy Loss: 0.3156812787055969, Cumulated Time: 70.19153685569763m \n",
      "Epoch:1-200/482, Train cross_entropy Loss: 0.3040069043636322, Cumulated Time: 74.68152325550714m \n",
      "Epoch:1-240/482, Train cross_entropy Loss: 0.42820918560028076, Cumulated Time: 79.02270950873692m \n",
      "Epoch:1-280/482, Train cross_entropy Loss: 0.5678292512893677, Cumulated Time: 83.20828889608383m \n",
      "Epoch:1-320/482, Train cross_entropy Loss: 0.25245869159698486, Cumulated Time: 87.48857242266337m \n",
      "Epoch:1-360/482, Train cross_entropy Loss: 0.5193339586257935, Cumulated Time: 91.90254491964976m \n",
      "Epoch:1-400/482, Train cross_entropy Loss: 0.2829855978488922, Cumulated Time: 96.29278106689453m \n",
      "Epoch:1-440/482, Train cross_entropy Loss: 0.2221924364566803, Cumulated Time: 100.59711436033248m \n",
      "Epoch:1-480/482, Train cross_entropy Loss: 0.07697881758213043, Cumulated Time: 105.02543723980585m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8898    0.9275    0.9083      2167\n",
      "           1     0.8284    0.7527    0.7888      1007\n",
      "\n",
      "    accuracy                         0.8721      3174\n",
      "   macro avg     0.8591    0.8401    0.8485      3174\n",
      "weighted avg     0.8703    0.8721    0.8704      3174\n",
      "\n",
      "Epoch : 1, weighted avg f1-score: 87.035 Acc : 87.209 on Test_set, Spend:7.978 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:1 Completed, Total Train Loss:192.5805612616241, Test Loss:61.355586502701044, Spend 113.09975444078445m \n",
      "Epoch:2-0/482, Train cross_entropy Loss: 0.3704628646373749, Cumulated Time: 113.27721908489863m \n",
      "Epoch:2-40/482, Train cross_entropy Loss: 0.29448020458221436, Cumulated Time: 117.47979239622752m \n",
      "Epoch:2-80/482, Train cross_entropy Loss: 0.23441043496131897, Cumulated Time: 121.81936817169189m \n",
      "Epoch:2-120/482, Train cross_entropy Loss: 0.16366370022296906, Cumulated Time: 126.15439108212789m \n",
      "Epoch:2-160/482, Train cross_entropy Loss: 0.180230051279068, Cumulated Time: 130.44164439837138m \n",
      "Epoch:2-200/482, Train cross_entropy Loss: 0.18641361594200134, Cumulated Time: 134.85734850565592m \n",
      "Epoch:2-240/482, Train cross_entropy Loss: 0.16948255896568298, Cumulated Time: 139.2270545999209m \n",
      "Epoch:2-280/482, Train cross_entropy Loss: 0.3987639248371124, Cumulated Time: 143.4501306295395m \n",
      "Epoch:2-320/482, Train cross_entropy Loss: 0.30274641513824463, Cumulated Time: 147.71869908571244m \n",
      "Epoch:2-360/482, Train cross_entropy Loss: 0.2177213579416275, Cumulated Time: 152.1450403849284m \n",
      "Epoch:2-400/482, Train cross_entropy Loss: 0.2802044451236725, Cumulated Time: 156.47654765844345m \n",
      "Epoch:2-440/482, Train cross_entropy Loss: 0.10227800905704498, Cumulated Time: 160.64111052354176m \n",
      "Epoch:2-480/482, Train cross_entropy Loss: 0.013196582905948162, Cumulated Time: 164.92699122826258m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9208    0.9391    0.9299      2167\n",
      "           1     0.8631    0.8262    0.8442      1007\n",
      "\n",
      "    accuracy                         0.9033      3174\n",
      "   macro avg     0.8919    0.8827    0.8871      3174\n",
      "weighted avg     0.9025    0.9033    0.9027      3174\n",
      "\n",
      "Epoch : 2, weighted avg f1-score: 90.270 Acc : 90.328 on Test_set, Spend:8.134 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:2 Completed, Total Train Loss:136.33069253060967, Test Loss:52.24798254389316, Spend 173.15608922640482m \n",
      "Epoch:3-0/482, Train cross_entropy Loss: 0.16949158906936646, Cumulated Time: 173.33488068183263m \n",
      "Epoch:3-40/482, Train cross_entropy Loss: 0.058115288615226746, Cumulated Time: 177.5813933332761m \n",
      "Epoch:3-80/482, Train cross_entropy Loss: 0.05770769715309143, Cumulated Time: 181.94629429976146m \n",
      "Epoch:3-120/482, Train cross_entropy Loss: 0.10673046857118607, Cumulated Time: 186.2607711037m \n",
      "Epoch:3-160/482, Train cross_entropy Loss: 0.1374599039554596, Cumulated Time: 190.45656047662098m \n",
      "Epoch:3-200/482, Train cross_entropy Loss: 0.1124999076128006, Cumulated Time: 194.70556181669235m \n",
      "Epoch:3-240/482, Train cross_entropy Loss: 0.08331742137670517, Cumulated Time: 199.03864078919094m \n",
      "Epoch:3-280/482, Train cross_entropy Loss: 0.22861133515834808, Cumulated Time: 203.29996542930604m \n",
      "Epoch:3-320/482, Train cross_entropy Loss: 0.15846870839595795, Cumulated Time: 207.5203849116961m \n",
      "Epoch:3-360/482, Train cross_entropy Loss: 0.149587482213974, Cumulated Time: 211.8361003001531m \n",
      "Epoch:3-400/482, Train cross_entropy Loss: 0.6274359822273254, Cumulated Time: 216.2552065650622m \n",
      "Epoch:3-440/482, Train cross_entropy Loss: 0.019552411511540413, Cumulated Time: 220.5319484313329m \n",
      "Epoch:3-480/482, Train cross_entropy Loss: 0.013142446987330914, Cumulated Time: 224.84648509820303m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9338    0.9511    0.9424      2167\n",
      "           1     0.8904    0.8550    0.8723      1007\n",
      "\n",
      "    accuracy                         0.9206      3174\n",
      "   macro avg     0.9121    0.9030    0.9074      3174\n",
      "weighted avg     0.9201    0.9206    0.9202      3174\n",
      "\n",
      "Epoch : 3, weighted avg f1-score: 92.016 Acc : 92.060 on Test_set, Spend:8.206 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:3 Completed, Total Train Loss:93.69398213364184, Test Loss:47.208372672321275, Spend 233.14659916559856m \n",
      "Epoch:4-0/482, Train cross_entropy Loss: 0.03943738341331482, Cumulated Time: 233.3313101410866m \n",
      "Epoch:4-40/482, Train cross_entropy Loss: 0.030062980949878693, Cumulated Time: 237.57859305143356m \n",
      "Epoch:4-80/482, Train cross_entropy Loss: 0.07226834446191788, Cumulated Time: 241.92290415366492m \n",
      "Epoch:4-120/482, Train cross_entropy Loss: 0.08431240916252136, Cumulated Time: 246.30525450309116m \n",
      "Epoch:4-160/482, Train cross_entropy Loss: 0.11356299370527267, Cumulated Time: 250.63478311300278m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4-200/482, Train cross_entropy Loss: 0.013004470616579056, Cumulated Time: 255.04421276251475m \n",
      "Epoch:4-240/482, Train cross_entropy Loss: 0.015415153466165066, Cumulated Time: 259.45934192736945m \n",
      "Epoch:4-280/482, Train cross_entropy Loss: 0.1666705161333084, Cumulated Time: 263.7700164437294m \n",
      "Epoch:4-320/482, Train cross_entropy Loss: 0.062098320573568344, Cumulated Time: 268.09861908753714m \n",
      "Epoch:4-360/482, Train cross_entropy Loss: 0.43453288078308105, Cumulated Time: 272.54982737700146m \n",
      "Epoch:4-400/482, Train cross_entropy Loss: 0.4472820460796356, Cumulated Time: 276.9300363063812m \n",
      "Epoch:4-440/482, Train cross_entropy Loss: 0.018767178058624268, Cumulated Time: 281.1562026460966m \n",
      "Epoch:4-480/482, Train cross_entropy Loss: 0.006271170452237129, Cumulated Time: 285.495251874129m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9565    0.9529    0.9547      2167\n",
      "           1     0.8995    0.9067    0.9031      1007\n",
      "\n",
      "    accuracy                         0.9382      3174\n",
      "   macro avg     0.9280    0.9298    0.9289      3174\n",
      "weighted avg     0.9384    0.9382    0.9383      3174\n",
      "\n",
      "Epoch : 4, weighted avg f1-score: 93.831 Acc : 93.825 on Test_set, Spend:8.127 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:4 Completed, Total Train Loss:64.16703297477216, Test Loss:42.942931096302345, Spend 293.71818452278774m \n",
      "Epoch:5-0/482, Train cross_entropy Loss: 0.01128471177071333, Cumulated Time: 293.8972340742747m \n",
      "Epoch:5-40/482, Train cross_entropy Loss: 0.060263197869062424, Cumulated Time: 298.1504718104998m \n",
      "Epoch:5-80/482, Train cross_entropy Loss: 0.02056446671485901, Cumulated Time: 302.49662771224973m \n",
      "Epoch:5-120/482, Train cross_entropy Loss: 0.03962545096874237, Cumulated Time: 306.7965572913488m \n",
      "Epoch:5-160/482, Train cross_entropy Loss: 0.017026877030730247, Cumulated Time: 311.0814442237218m \n",
      "Epoch:5-200/482, Train cross_entropy Loss: 0.014763268642127514, Cumulated Time: 315.4937727530797m \n",
      "Epoch:5-240/482, Train cross_entropy Loss: 0.017988646402955055, Cumulated Time: 319.8374366044998m \n",
      "Epoch:5-280/482, Train cross_entropy Loss: 0.22521395981311798, Cumulated Time: 324.0567857782046m \n",
      "Epoch:5-320/482, Train cross_entropy Loss: 0.0832761824131012, Cumulated Time: 328.3676900903384m \n",
      "Epoch:5-360/482, Train cross_entropy Loss: 0.13794709742069244, Cumulated Time: 332.79599705934527m \n",
      "Epoch:5-400/482, Train cross_entropy Loss: 0.3950619101524353, Cumulated Time: 337.1689767360687m \n",
      "Epoch:5-440/482, Train cross_entropy Loss: 0.029542390257120132, Cumulated Time: 341.4819012006124m \n",
      "Epoch:5-480/482, Train cross_entropy Loss: 0.31112030148506165, Cumulated Time: 345.90019854704536m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9566    0.9672    0.9619      2167\n",
      "           1     0.9278    0.9057    0.9166      1007\n",
      "\n",
      "    accuracy                         0.9477      3174\n",
      "   macro avg     0.9422    0.9364    0.9392      3174\n",
      "weighted avg     0.9475    0.9477    0.9475      3174\n",
      "\n",
      "Epoch : 5, weighted avg f1-score: 94.753 Acc : 94.770 on Test_set, Spend:7.953 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:5 Completed, Total Train Loss:44.740484988782555, Test Loss:41.68718559842091, Spend 353.9493539055189m \n",
      "Epoch:6-0/482, Train cross_entropy Loss: 0.06772288680076599, Cumulated Time: 354.1218447724978m \n",
      "Epoch:6-40/482, Train cross_entropy Loss: 0.010540436021983624, Cumulated Time: 358.32698468764625m \n",
      "Epoch:6-80/482, Train cross_entropy Loss: 0.09448245167732239, Cumulated Time: 362.6397514065107m \n",
      "Epoch:6-120/482, Train cross_entropy Loss: 0.013678625226020813, Cumulated Time: 366.87568495670956m \n",
      "Epoch:6-160/482, Train cross_entropy Loss: 0.01795436628162861, Cumulated Time: 371.04867145220436m \n",
      "Epoch:6-200/482, Train cross_entropy Loss: 0.011070596985518932, Cumulated Time: 375.3238460699717m \n",
      "Epoch:6-240/482, Train cross_entropy Loss: 0.016156289726495743, Cumulated Time: 379.67430572112403m \n",
      "Epoch:6-280/482, Train cross_entropy Loss: 0.057633232325315475, Cumulated Time: 383.9256601333618m \n",
      "Epoch:6-320/482, Train cross_entropy Loss: 0.007959785871207714, Cumulated Time: 388.18746826648714m \n",
      "Epoch:6-360/482, Train cross_entropy Loss: 0.03954407945275307, Cumulated Time: 392.5979612112045m \n",
      "Epoch:6-400/482, Train cross_entropy Loss: 0.08952251821756363, Cumulated Time: 397.0184982776642m \n",
      "Epoch:6-440/482, Train cross_entropy Loss: 0.03088710829615593, Cumulated Time: 401.2827538251877m \n",
      "Epoch:6-480/482, Train cross_entropy Loss: 0.002831128891557455, Cumulated Time: 405.64453089634577m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9581    0.9700    0.9640      2167\n",
      "           1     0.9337    0.9086    0.9210      1007\n",
      "\n",
      "    accuracy                         0.9505      3174\n",
      "   macro avg     0.9459    0.9393    0.9425      3174\n",
      "weighted avg     0.9503    0.9505    0.9504      3174\n",
      "\n",
      "Epoch : 6, weighted avg f1-score: 95.035 Acc : 95.054 on Test_set, Spend:8.204 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:6 Completed, Total Train Loss:32.189756359672174, Test Loss:42.51107707875781, Spend 413.9455268939336m \n",
      "Epoch:7-0/482, Train cross_entropy Loss: 0.010213182307779789, Cumulated Time: 414.1212218046188m \n",
      "Epoch:7-40/482, Train cross_entropy Loss: 0.011314894072711468, Cumulated Time: 418.38845296700794m \n",
      "Epoch:7-80/482, Train cross_entropy Loss: 0.01150492113083601, Cumulated Time: 422.77921863794325m \n",
      "Epoch:7-120/482, Train cross_entropy Loss: 0.07540751248598099, Cumulated Time: 427.08587490320207m \n",
      "Epoch:7-160/482, Train cross_entropy Loss: 0.005418024025857449, Cumulated Time: 431.2913664102554m \n",
      "Epoch:7-200/482, Train cross_entropy Loss: 0.0036206869408488274, Cumulated Time: 435.5914924184481m \n",
      "Epoch:7-240/482, Train cross_entropy Loss: 0.04190387949347496, Cumulated Time: 439.96803996165596m \n",
      "Epoch:7-280/482, Train cross_entropy Loss: 0.01355104148387909, Cumulated Time: 444.2369391004244m \n",
      "Epoch:7-320/482, Train cross_entropy Loss: 0.01593777723610401, Cumulated Time: 448.4387495915095m \n",
      "Epoch:7-360/482, Train cross_entropy Loss: 0.33072254061698914, Cumulated Time: 452.7238524278005m \n",
      "Epoch:7-400/482, Train cross_entropy Loss: 0.02569981850683689, Cumulated Time: 457.13824850320816m \n",
      "Epoch:7-440/482, Train cross_entropy Loss: 0.009959348477423191, Cumulated Time: 461.46097826162975m \n",
      "Epoch:7-480/482, Train cross_entropy Loss: 0.0028782500885427, Cumulated Time: 465.77571153243383m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9577    0.9723    0.9650      2167\n",
      "           1     0.9384    0.9076    0.9228      1007\n",
      "\n",
      "    accuracy                         0.9518      3174\n",
      "   macro avg     0.9481    0.9400    0.9439      3174\n",
      "weighted avg     0.9516    0.9518    0.9516      3174\n",
      "\n",
      "Epoch : 7, weighted avg f1-score: 95.158 Acc : 95.180 on Test_set, Spend:8.207 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:7 Completed, Total Train Loss:24.432538666180335, Test Loss:43.562897340161726, Spend 474.07766998608906m \n",
      "Epoch:8-0/482, Train cross_entropy Loss: 0.47511807084083557, Cumulated Time: 474.2525073607763m \n",
      "Epoch:8-40/482, Train cross_entropy Loss: 0.014720505103468895, Cumulated Time: 478.4465541799863m \n",
      "Epoch:8-80/482, Train cross_entropy Loss: 0.015270477160811424, Cumulated Time: 482.6164073308309m \n",
      "Epoch:8-120/482, Train cross_entropy Loss: 0.009155193343758583, Cumulated Time: 486.90553172032037m \n",
      "Epoch:8-160/482, Train cross_entropy Loss: 0.0033046177122741938, Cumulated Time: 491.2232299844424m \n",
      "Epoch:8-200/482, Train cross_entropy Loss: 0.004886625334620476, Cumulated Time: 495.47126796642937m \n",
      "Epoch:8-240/482, Train cross_entropy Loss: 0.005759356543421745, Cumulated Time: 499.6698704083761m \n",
      "Epoch:8-280/482, Train cross_entropy Loss: 0.006750450935214758, Cumulated Time: 503.92590804894763m \n",
      "Epoch:8-320/482, Train cross_entropy Loss: 0.01252424344420433, Cumulated Time: 508.2737353523572m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8-360/482, Train cross_entropy Loss: 0.02676006220281124, Cumulated Time: 512.6303454359372m \n",
      "Epoch:8-400/482, Train cross_entropy Loss: 0.04516971856355667, Cumulated Time: 517.025889090697m \n",
      "Epoch:8-440/482, Train cross_entropy Loss: 0.010157848708331585, Cumulated Time: 521.387894153595m \n",
      "Epoch:8-480/482, Train cross_entropy Loss: 0.004736334551125765, Cumulated Time: 525.7387847940128m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9631    0.9645    0.9638      2167\n",
      "           1     0.9233    0.9206    0.9219      1007\n",
      "\n",
      "    accuracy                         0.9505      3174\n",
      "   macro avg     0.9432    0.9425    0.9429      3174\n",
      "weighted avg     0.9505    0.9505    0.9505      3174\n",
      "\n",
      "Epoch : 8, weighted avg f1-score: 95.052 Acc : 95.054 on Test_set, Spend:8.155 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:8 Completed, Total Train Loss:20.19952734466642, Test Loss:43.766828666906804, Spend 533.9884021957715m \n"
     ]
    }
   ],
   "source": [
    "optimizer = BertAdam(model.parameters(), lr=config_learning_rate0,\n",
    "                     warmup=config_warmup_proportion, t_total=total_train_steps, weight_decay=config_l2_decay)\n",
    "\n",
    "train_start = time.time()\n",
    "global_step_th = int(len(train_examples) / batch_size /\n",
    "                     gradient_accumulation_steps * start_epoch)\n",
    "\n",
    "all_loss_list = {'train': [], 'test': []}\n",
    "all_f1_list = {'train': [], 'test': []}\n",
    "for epoch in range(start_epoch, total_train_epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if prev_save_step > -1:\n",
    "            if step <= prev_save_step:\n",
    "                continue\n",
    "        if prev_save_step > -1:\n",
    "            prev_save_step = -1\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "\n",
    "        _, logits = model(gcn_adj_list, gcn_swop_eye,\n",
    "                          input_ids, segment_ids, input_mask)\n",
    "\n",
    "        if config_loss_criterion == 'mse':\n",
    "            if do_softmax_before_mse:\n",
    "                logits = F.softmax(logits, -1)\n",
    "            loss = F.mse_loss(logits, y_prob)\n",
    "        else:\n",
    "            if loss_weight is None:\n",
    "                loss = F.cross_entropy(logits, label_ids)\n",
    "            else:\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, num_classes), label_ids, loss_weight)\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step_th += 1\n",
    "        if step % 40 == 0:\n",
    "            print(\"Epoch:{}-{}/{}, Train {} Loss: {}, Cumulated Time: {}m \".format(epoch, step,\n",
    "                  len(train_dataloader), config_loss_criterion, loss.item(), (time.time() - train_start)/60.0))\n",
    "\n",
    "    print('*' * 50)\n",
    "    test_loss, test_acc, perform_metrics = evaluate(model, gcn_adj_list, test_dataloader, epoch, 'Test_set')\n",
    "    all_loss_list['train'].append(train_loss)\n",
    "    all_loss_list['test'].append(test_loss)\n",
    "    all_f1_list['test'].append(perform_metrics)\n",
    "    print(\"Epoch:{} Completed, Total Train Loss:{}, Test Loss:{}, Spend {}m \".format(\n",
    "        epoch, train_loss, test_loss, (time.time() - train_start) / 60.0))\n",
    "\n",
    "    if perform_metrics > perform_metrics_prev:\n",
    "        to_save = {'epoch': epoch, 'model_state': model.state_dict(),\n",
    "                   'valid_acc': test_acc, 'lower_case': do_lower_case,\n",
    "                   'perform_metrics': perform_metrics}\n",
    "        torch.save(to_save, os.path.join(output_dir, model_file_save))\n",
    "\n",
    "        perform_metrics_prev = perform_metrics\n",
    "\n",
    "        valid_f1_best_epoch = epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed9b61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished! Total Spend Time: 533.9885460972786\n",
      "Test Weighted F1: 95.158 at 7 Epoch.\n"
     ]
    }
   ],
   "source": [
    "print('Optimization Finished! Total Spend Time:', (time.time() - train_start)/60.0)\n",
    "print('Test Weighted F1: %.3f at %d Epoch.' % (100 * perform_metrics_prev, valid_f1_best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec3f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
