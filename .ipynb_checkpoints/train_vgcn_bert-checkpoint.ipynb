{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f325a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam  # , warmup_linear\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from model_vgcn_bert import VGCN_Bert\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9c6f6",
   "metadata": {},
   "source": [
    "#### Step 1:   Configurations for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d47f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 1: CONFIGURATIONS FOR TRAINING--------\n",
      "Dataset:  pheme\n",
      "Will Load Model from Checkpoint:  False\n",
      "Will Delete Stop Words:  True\n",
      "Vocab GCN Hidden Dim: vocab_size -> 128 -> 16\n",
      "Learning Rate0:  1e-05\n",
      "Weight Decay:  0.01\n",
      "Loss Criterion:  cross_entropy\n",
      "Will Perform Softmax before MSE:  True\n",
      "Vocab Adjcent:  all\n",
      "MAX_SEQ_LENGTH:  216\n",
      "Perform Metrics:  ['weighted avg', 'f1-score']\n",
      "Saved Model File Name:  VGCN_BERT16_model_pheme_cross_entropy_sw1.pt\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# # ds = dataset, sw = stopwords, lr = learning rate, l2 = L2 regularization\n",
    "# parser.add_argument('--ds', type=str, default='pheme')\n",
    "# parser.add_argument('--load', type=int, default='0')\n",
    "# parser.add_argument('--sw', type=int, default='1')\n",
    "# parser.add_argument('--dim', type=int, default='16')\n",
    "# parser.add_argument('--lr', type=float, default=1e-5)  # 2e-5\n",
    "# parser.add_argument('--l2', type=float, default=0.01)  # 0.001\n",
    "# parser.add_argument('--model', type=str, default='VGCN_BERT')\n",
    "# args = parser.parse_args()\n",
    "args = {\"ds\": \"pheme\", \"load\": 0, \"sw\": 1, \"dim\": 16,\n",
    "        \"lr\": 1e-5, \"l2\": 0.01, \"model\": \"VGCN_BERT\"}\n",
    "\n",
    "# config_dataset = args.ds\n",
    "# config_load_model_from_checkpoint = True if args.load == 1 else False\n",
    "# config_use_stopwords = True if args.sw == 1 else False\n",
    "# config_gcn_embedding_dim = args.dim\n",
    "# config_learning_rate0 = args.lr\n",
    "# config_l2_decay = args.l2\n",
    "# config_model_type = args.model\n",
    "\n",
    "config_dataset = args[\"ds\"]\n",
    "config_load_model_from_checkpoint = True if args[\"load\"] == 1 else False\n",
    "config_use_stopwords = True if args[\"sw\"] == 1 else False\n",
    "config_gcn_embedding_dim = args[\"dim\"]\n",
    "config_learning_rate0 = args[\"lr\"]\n",
    "config_l2_decay = args[\"l2\"]\n",
    "config_model_type = args[\"model\"]\n",
    "\n",
    "config_warmup_proportion = 0.1\n",
    "config_vocab_adj = 'all'  # pmi / tf / all\n",
    "config_adj_npmi_threshold = 0.2\n",
    "config_adj_tf_threshold = 0\n",
    "config_loss_criterion = 'cross_entropy'\n",
    "\n",
    "MAX_SEQ_LENGTH = 200 + config_gcn_embedding_dim\n",
    "total_train_epochs = 9\n",
    "batch_size = 16  # 12\n",
    "gradient_accumulation_steps = 1\n",
    "bert_model_scale = 'bert-base-uncased'\n",
    "do_lower_case = True\n",
    "perform_metrics_str = ['weighted avg', 'f1-score']\n",
    "do_softmax_before_mse = True\n",
    "\n",
    "data_dir = './prepared_data/'\n",
    "output_dir = './model_output/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "model_file_save = config_model_type + str(config_gcn_embedding_dim) + '_model_' + \\\n",
    "    config_dataset + '_' + config_loss_criterion + '_' + \\\n",
    "    \"sw\" + str(int(config_use_stopwords)) + '.pt'\n",
    "\n",
    "print('----------STEP 1: CONFIGURATIONS FOR TRAINING--------')\n",
    "print('Dataset: ', config_dataset)\n",
    "print('Will Load Model from Checkpoint: ', config_load_model_from_checkpoint)\n",
    "print('Will Delete Stop Words: ', config_use_stopwords)\n",
    "print('Vocab GCN Hidden Dim: vocab_size -> 128 -> ' + str(config_gcn_embedding_dim))\n",
    "print('Learning Rate0: ', config_learning_rate0)\n",
    "print('Weight Decay: ', config_l2_decay)\n",
    "print('Loss Criterion: ', config_loss_criterion)\n",
    "print('Will Perform Softmax before MSE: ', do_softmax_before_mse)\n",
    "print('Vocab Adjcent: ', config_vocab_adj)\n",
    "print('MAX_SEQ_LENGTH: ', MAX_SEQ_LENGTH)\n",
    "print('Perform Metrics: ', perform_metrics_str)\n",
    "print('Saved Model File Name: ', model_file_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fab5ac",
   "metadata": {},
   "source": [
    "#### Step 2.1: Prepare Dataset & Load Vocabulary Adjacent Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbc1a634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------\n",
      " Load and seperate pheme dataset, with vocabulary graph adjacent matrix\n"
     ]
    }
   ],
   "source": [
    "print('----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------')\n",
    "print(' Load and seperate', config_dataset, 'dataset, with vocabulary graph adjacent matrix')\n",
    "\n",
    "objects = []\n",
    "names = ['index_label', 'train_label', 'train_label_prob', 'test_label',\n",
    "         'test_label_prob', 'clean_docs', 'vocab_adj_tf', 'vocab_adj_pmi', 'vocab_map']\n",
    "\n",
    "for i in range(len(names)):\n",
    "    datafile = data_dir + \"/data_%s.%s\" % (config_dataset, names[i])\n",
    "    with open(datafile, 'rb') as f:\n",
    "        objects.append(pkl.load(f, encoding='latin1'))\n",
    "\n",
    "index_labels_list, train_label, train_label_prob, test_label, test_label_prob, shuffled_clean_docs, gcn_vocab_adj_tf, gcn_vocab_adj_pmi, gcn_vocab_map = tuple(objects)\n",
    "\n",
    "label2idx = index_labels_list[0]\n",
    "idx2label = index_labels_list[1]\n",
    "\n",
    "all_labels = np.hstack((train_label, test_label))\n",
    "all_labels_prob = np.vstack((train_label_prob, test_label_prob))\n",
    "\n",
    "examples = []\n",
    "for i, text in enumerate(shuffled_clean_docs):\n",
    "    example = InputExample(i, text.strip(), confidence=all_labels_prob[i], label=all_labels[i])\n",
    "    examples.append(example)\n",
    "\n",
    "num_classes = len(label2idx)\n",
    "gcn_vocab_size = len(gcn_vocab_map)\n",
    "train_size = len(train_label)\n",
    "test_size = len(test_label)\n",
    "\n",
    "indexs = np.arange(0, len(examples))\n",
    "train_examples = [examples[i] for i in indexs[:train_size]]\n",
    "test_examples = [examples[i] for i in indexs[train_size:train_size + test_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "363fd7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero ratio for vocab adj 0th: 77.09113559\n",
      "Zero ratio for vocab adj 1th: 94.97669180\n"
     ]
    }
   ],
   "source": [
    "if config_adj_tf_threshold > 0:\n",
    "    gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > config_adj_tf_threshold)\n",
    "    gcn_vocab_adj_tf.eliminate_zeros()\n",
    "if config_adj_npmi_threshold > 0:\n",
    "    gcn_vocab_adj_pmi.data *= (gcn_vocab_adj_pmi.data > config_adj_npmi_threshold)\n",
    "    gcn_vocab_adj_pmi.eliminate_zeros()\n",
    "\n",
    "if config_vocab_adj == 'pmi':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_pmi]\n",
    "elif config_vocab_adj == 'tf':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf]\n",
    "elif config_vocab_adj == 'all':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf, gcn_vocab_adj_pmi]\n",
    "\n",
    "norm_gcn_vocab_adj_list = []\n",
    "for i in range(len(gcn_vocab_adj_list)):\n",
    "    adj = gcn_vocab_adj_list[i]\n",
    "\n",
    "    print('Zero ratio for vocab adj %dth: %.8f' %\n",
    "          (i, 100 * (1 - adj.count_nonzero() / (adj.shape[0] * adj.shape[1]))))\n",
    "\n",
    "    adj = normalize_adj(adj)\n",
    "    norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
    "\n",
    "gcn_adj_list = norm_gcn_vocab_adj_list\n",
    "\n",
    "\n",
    "train_classes_num, train_classes_weight = get_class_count_and_weight(train_label, len(label2idx))\n",
    "loss_weight = torch.tensor(train_classes_weight).to(device)\n",
    "loss_weight = torch.tensor(loss_weight, dtype=torch.float32).to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e606fcc",
   "metadata": {},
   "source": [
    "#### Step 2.2:   Prepare PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ed4db8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classes Count:  [5341, 2368]\n",
      "Batch size:  16\n",
      "Num steps:  4338\n",
      "Number of Examples for Training:  7709\n",
      "Number of Examples for Training After Dataloader:  7712\n",
      "Number of Examples for Validate:  1647\n"
     ]
    }
   ],
   "source": [
    "def get_pytorch_dataloader(examples, tokenizer, batch_size):\n",
    "    dataset = CorpusDataset(examples, tokenizer, gcn_vocab_map, MAX_SEQ_LENGTH, config_gcn_embedding_dim)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=dataset.pad)\n",
    "\n",
    "\n",
    "train_dataloader = get_pytorch_dataloader(train_examples, tokenizer, batch_size)\n",
    "test_dataloader = get_pytorch_dataloader(test_examples, tokenizer, batch_size)\n",
    "\n",
    "total_train_steps = int(len(train_dataloader) / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print('Train Classes Count: ', train_classes_num)\n",
    "print('Batch size: ', batch_size)\n",
    "print('Num steps: ', total_train_steps)\n",
    "print('Number of Examples for Training: ', len(train_examples))\n",
    "print('Number of Examples for Training After Dataloader: ', len(train_dataloader) * batch_size)\n",
    "print('Number of Examples for Validate: ', len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d32200",
   "metadata": {},
   "source": [
    "#### Step 3.1:  Define Evaluating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc8f26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, examples, tokenizer, batch_size):\n",
    "    dataloader = get_pytorch_dataloader(examples, tokenizer, batch_size)\n",
    "    predict_out = []\n",
    "    confidence_out = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, _, _, gcn_swop_eye = batch\n",
    "\n",
    "            _, score_out = model(gcn_adj_list, gcn_swop_eye,\n",
    "                                 input_ids, segment_ids, input_mask)\n",
    "            if config_loss_criterion == 'mse' and do_softmax_before_mse:\n",
    "                score_out = torch.nn.functional.softmax(score_out, dim=-1)\n",
    "            predict_out.extend(score_out.max(1)[1].tolist())\n",
    "            confidence_out.extend(score_out.max(1)[0].tolist())\n",
    "\n",
    "    return np.array(predict_out).reshape(-1), np.array(confidence_out).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "865ad863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, gcn_adj_list, predict_dataloader, epoch_th, dataset_name):\n",
    "    model.eval()\n",
    "    predict_out = []\n",
    "    all_label_ids = []\n",
    "    ev_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in predict_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            _, logits = model(gcn_adj_list, gcn_swop_eye,input_ids,  segment_ids, input_mask)\n",
    "\n",
    "            if config_loss_criterion == 'mse':\n",
    "                if do_softmax_before_mse:\n",
    "                    logits = F.softmax(logits, -1)\n",
    "                loss = F.mse_loss(logits, y_prob)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "            ev_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, -1)\n",
    "            predict_out.extend(predicted.tolist())\n",
    "            all_label_ids.extend(label_ids.tolist())\n",
    "            eval_accuracy = predicted.eq(label_ids).sum().item()\n",
    "            total += len(label_ids)\n",
    "            correct += eval_accuracy\n",
    "\n",
    "        f1_metrics = f1_score(np.array(all_label_ids).reshape(-1), np.array(predict_out).reshape(-1), average='weighted')\n",
    "        print(\"Report:\\n\"+classification_report(np.array(all_label_ids).reshape(-1), np.array(predict_out).reshape(-1), digits=4))\n",
    "\n",
    "    ev_acc = correct/total\n",
    "    end = time.time()\n",
    "    print('Epoch : %d, %s: %.3f Acc : %.3f on %s, Spend:%.3f minutes for evaluation'\n",
    "          % (epoch_th, ' '.join(perform_metrics_str), 100 * f1_metrics, 100. * ev_acc, dataset_name, (end - start) / 60.0))\n",
    "    print('*' * 50)\n",
    "    return ev_loss, ev_acc, f1_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0f581",
   "metadata": {},
   "source": [
    "#### Step 3.2:   Load / Initialize VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77bd8c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 3: START TRAINING VGCN_BERT MODEL----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGCN_Bert(\n",
       "  (embeddings): VGCNBertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (vocab_gcn): VocabGraphConvolution(\n",
       "      (fc_hc): Linear(in_features=128, out_features=16, bias=True)\n",
       "      (act_func): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----------STEP 3: START TRAINING VGCN_BERT MODEL----------')\n",
    "\n",
    "if config_load_model_from_checkpoint and os.path.exists(os.path.join(output_dir, model_file_save)):\n",
    "    checkpoint = torch.load(os.path.join(output_dir, model_file_save), map_location='cpu')\n",
    "    if 'step' in checkpoint:\n",
    "        prev_save_step = checkpoint['step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        prev_save_step = -1\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    valid_acc_prev = checkpoint['valid_acc']\n",
    "    perform_metrics_prev = checkpoint['perform_metrics']\n",
    "    model = VGCN_Bert.from_pretrained(bert_model_scale, state_dict=checkpoint['model_state'], gcn_adj_dim=gcn_vocab_size, \n",
    "        gcn_adj_num=len(gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "\n",
    "    pretrained_dict = checkpoint['model_state']\n",
    "    net_state_dict = model.state_dict()\n",
    "    pretrained_dict_selected = {\n",
    "        k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "    net_state_dict.update(pretrained_dict_selected)\n",
    "    model.load_state_dict(net_state_dict)\n",
    "\n",
    "    print('Loaded the pretrain model:', model_file_save, ', epoch:', checkpoint['epoch'], 'step:', prev_save_step, 'valid acc:',\n",
    "          checkpoint['valid_acc'], ' '.join(perform_metrics_str)+'_valid:', checkpoint['perform_metrics'])\n",
    "\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    valid_acc_prev = 0\n",
    "    perform_metrics_prev = 0\n",
    "    model = VGCN_Bert.from_pretrained(bert_model_scale, gcn_adj_dim=gcn_vocab_size, gcn_adj_num=len(\n",
    "        gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "    prev_save_step = -1\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f99d6",
   "metadata": {},
   "source": [
    "#### Step 3.3:   Start Training VGCN_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "040c2068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0-0/482, Train cross_entropy Loss: 0.837292492389679, Cumulated Time: 0.10365605354309082m \n",
      "Epoch:0-40/482, Train cross_entropy Loss: 0.6853340268135071, Cumulated Time: 4.137483894824982m \n",
      "Epoch:0-80/482, Train cross_entropy Loss: 0.734981119632721, Cumulated Time: 8.531713342666626m \n",
      "Epoch:0-120/482, Train cross_entropy Loss: 0.636677086353302, Cumulated Time: 13.00891508658727m \n",
      "Epoch:0-160/482, Train cross_entropy Loss: 0.3965364396572113, Cumulated Time: 17.543501615524292m \n",
      "Epoch:0-200/482, Train cross_entropy Loss: 0.43856096267700195, Cumulated Time: 22.141229939460754m \n",
      "Epoch:0-240/482, Train cross_entropy Loss: 0.6776555776596069, Cumulated Time: 26.62836802403132m \n",
      "Epoch:0-280/482, Train cross_entropy Loss: 0.6454180479049683, Cumulated Time: 31.119011239210764m \n",
      "Epoch:0-320/482, Train cross_entropy Loss: 0.2579997777938843, Cumulated Time: 35.64641051292419m \n",
      "Epoch:0-360/482, Train cross_entropy Loss: 0.6978713870048523, Cumulated Time: 40.203782844543454m \n",
      "Epoch:0-400/482, Train cross_entropy Loss: 0.42831507325172424, Cumulated Time: 44.859695784250896m \n",
      "Epoch:0-440/482, Train cross_entropy Loss: 0.3178236782550812, Cumulated Time: 49.444438894589744m \n",
      "Epoch:0-480/482, Train cross_entropy Loss: 0.2322392761707306, Cumulated Time: 53.971388634045915m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8208    0.8922    0.8550      1104\n",
      "           1     0.7338    0.6041    0.6626       543\n",
      "\n",
      "    accuracy                         0.7972      1647\n",
      "   macro avg     0.7773    0.7481    0.7588      1647\n",
      "weighted avg     0.7921    0.7972    0.7916      1647\n",
      "\n",
      "Epoch : 0, weighted avg f1-score: 79.160 Acc : 79.721 on Test_set, Spend:4.311 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:0 Completed, Total Train Loss:268.16887284070253, Test Loss:45.77485961467028, Spend 58.37944280703862m \n",
      "Epoch:1-0/482, Train cross_entropy Loss: 0.9134872555732727, Cumulated Time: 58.55193320115407m \n",
      "Epoch:1-40/482, Train cross_entropy Loss: 0.4319554269313812, Cumulated Time: 63.009449712435405m \n",
      "Epoch:1-80/482, Train cross_entropy Loss: 0.47027748823165894, Cumulated Time: 67.43029479583105m \n",
      "Epoch:1-120/482, Train cross_entropy Loss: 0.2508128881454468, Cumulated Time: 71.88195779323578m \n",
      "Epoch:1-160/482, Train cross_entropy Loss: 0.2201365977525711, Cumulated Time: 76.34988046089808m \n",
      "Epoch:1-200/482, Train cross_entropy Loss: 0.2788609564304352, Cumulated Time: 80.76594837903977m \n",
      "Epoch:1-240/482, Train cross_entropy Loss: 0.5163430571556091, Cumulated Time: 85.08272261619568m \n",
      "Epoch:1-280/482, Train cross_entropy Loss: 0.6189031004905701, Cumulated Time: 89.43138630787531m \n",
      "Epoch:1-320/482, Train cross_entropy Loss: 0.2972652316093445, Cumulated Time: 93.81670915285746m \n",
      "Epoch:1-360/482, Train cross_entropy Loss: 0.5124358534812927, Cumulated Time: 98.13882938623428m \n",
      "Epoch:1-400/482, Train cross_entropy Loss: 0.25940608978271484, Cumulated Time: 102.47789749701818m \n",
      "Epoch:1-440/482, Train cross_entropy Loss: 0.2160700261592865, Cumulated Time: 106.84736331701279m \n",
      "Epoch:1-480/482, Train cross_entropy Loss: 0.052444927394390106, Cumulated Time: 111.23735684156418m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8696    0.9004    0.8847      1104\n",
      "           1     0.7817    0.7256    0.7526       543\n",
      "\n",
      "    accuracy                         0.8427      1647\n",
      "   macro avg     0.8257    0.8130    0.8187      1647\n",
      "weighted avg     0.8407    0.8427    0.8412      1647\n",
      "\n",
      "Epoch : 1, weighted avg f1-score: 84.118 Acc : 84.274 on Test_set, Spend:4.078 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:1 Completed, Total Train Loss:189.52478532493114, Test Loss:39.6340609844774, Spend 115.40864696105321m \n",
      "Epoch:2-0/482, Train cross_entropy Loss: 0.2102990746498108, Cumulated Time: 115.5792671362559m \n",
      "Epoch:2-40/482, Train cross_entropy Loss: 0.2613069713115692, Cumulated Time: 119.81333003838857m \n",
      "Epoch:2-80/482, Train cross_entropy Loss: 0.20361393690109253, Cumulated Time: 124.20797908703486m \n",
      "Epoch:2-120/482, Train cross_entropy Loss: 0.20038250088691711, Cumulated Time: 128.58144438266754m \n",
      "Epoch:2-160/482, Train cross_entropy Loss: 0.12955841422080994, Cumulated Time: 132.84398957888286m \n",
      "Epoch:2-200/482, Train cross_entropy Loss: 0.20760321617126465, Cumulated Time: 137.20242559512457m \n",
      "Epoch:2-240/482, Train cross_entropy Loss: 0.20768243074417114, Cumulated Time: 141.6393770813942m \n",
      "Epoch:2-280/482, Train cross_entropy Loss: 0.5075989961624146, Cumulated Time: 146.013856391112m \n",
      "Epoch:2-320/482, Train cross_entropy Loss: 0.20582950115203857, Cumulated Time: 150.4065687219302m \n",
      "Epoch:2-360/482, Train cross_entropy Loss: 0.7436756491661072, Cumulated Time: 154.93611586093903m \n",
      "Epoch:2-400/482, Train cross_entropy Loss: 0.2546093165874481, Cumulated Time: 159.50459771156312m \n",
      "Epoch:2-440/482, Train cross_entropy Loss: 0.16455572843551636, Cumulated Time: 163.92394389708838m \n",
      "Epoch:2-480/482, Train cross_entropy Loss: 0.026030156761407852, Cumulated Time: 168.45442832708358m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8941    0.9103    0.9022      1104\n",
      "           1     0.8107    0.7808    0.7955       543\n",
      "\n",
      "    accuracy                         0.8676      1647\n",
      "   macro avg     0.8524    0.8456    0.8488      1647\n",
      "weighted avg     0.8666    0.8676    0.8670      1647\n",
      "\n",
      "Epoch : 2, weighted avg f1-score: 86.699 Acc : 86.764 on Test_set, Spend:4.333 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:2 Completed, Total Train Loss:134.0958615615964, Test Loss:40.71734630595893, Spend 172.8860117872556m \n",
      "Epoch:3-0/482, Train cross_entropy Loss: 0.04247754439711571, Cumulated Time: 173.06107233762742m \n",
      "Epoch:3-40/482, Train cross_entropy Loss: 0.08234219998121262, Cumulated Time: 177.38269881010055m \n",
      "Epoch:3-80/482, Train cross_entropy Loss: 0.137430801987648, Cumulated Time: 181.68328348795572m \n",
      "Epoch:3-120/482, Train cross_entropy Loss: 0.09617616981267929, Cumulated Time: 186.0741244753202m \n",
      "Epoch:3-160/482, Train cross_entropy Loss: 0.044793952256441116, Cumulated Time: 190.5240816473961m \n",
      "Epoch:3-200/482, Train cross_entropy Loss: 0.0888698548078537, Cumulated Time: 194.97728816270828m \n",
      "Epoch:3-240/482, Train cross_entropy Loss: 0.08102668076753616, Cumulated Time: 199.42421858708065m \n",
      "Epoch:3-280/482, Train cross_entropy Loss: 0.48802730441093445, Cumulated Time: 203.87535824775696m \n",
      "Epoch:3-320/482, Train cross_entropy Loss: 0.09263221174478531, Cumulated Time: 208.23915774027506m \n",
      "Epoch:3-360/482, Train cross_entropy Loss: 0.3571593463420868, Cumulated Time: 212.60270479917526m \n",
      "Epoch:3-400/482, Train cross_entropy Loss: 0.19521890580654144, Cumulated Time: 217.0550960302353m \n",
      "Epoch:3-440/482, Train cross_entropy Loss: 0.06584233045578003, Cumulated Time: 221.48611409664153m \n",
      "Epoch:3-480/482, Train cross_entropy Loss: 0.008610019460320473, Cumulated Time: 225.91648997863135m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8983    0.9203    0.9092      1104\n",
      "           1     0.8295    0.7882    0.8083       543\n",
      "\n",
      "    accuracy                         0.8767      1647\n",
      "   macro avg     0.8639    0.8543    0.8587      1647\n",
      "weighted avg     0.8756    0.8767    0.8759      1647\n",
      "\n",
      "Epoch : 3, weighted avg f1-score: 87.592 Acc : 87.675 on Test_set, Spend:4.244 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:3 Completed, Total Train Loss:89.96855128742754, Test Loss:44.085749863879755, Spend 230.25532463788986m \n",
      "Epoch:4-0/482, Train cross_entropy Loss: 0.10069230198860168, Cumulated Time: 230.43052724202474m \n",
      "Epoch:4-40/482, Train cross_entropy Loss: 0.0482955276966095, Cumulated Time: 234.87673743168514m \n",
      "Epoch:4-80/482, Train cross_entropy Loss: 0.07539504766464233, Cumulated Time: 239.31132600307464m \n",
      "Epoch:4-120/482, Train cross_entropy Loss: 0.1281915009021759, Cumulated Time: 243.74533803860348m \n",
      "Epoch:4-160/482, Train cross_entropy Loss: 0.03317629545927048, Cumulated Time: 248.26106944878896m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4-200/482, Train cross_entropy Loss: 0.034823860973119736, Cumulated Time: 252.83579027255377m \n",
      "Epoch:4-240/482, Train cross_entropy Loss: 0.015927745029330254, Cumulated Time: 257.325769384702m \n",
      "Epoch:4-280/482, Train cross_entropy Loss: 0.26326310634613037, Cumulated Time: 261.8048376361529m \n",
      "Epoch:4-320/482, Train cross_entropy Loss: 0.029870448634028435, Cumulated Time: 266.3347892284393m \n",
      "Epoch:4-360/482, Train cross_entropy Loss: 0.37975481152534485, Cumulated Time: 270.8170592904091m \n",
      "Epoch:4-400/482, Train cross_entropy Loss: 0.0750889778137207, Cumulated Time: 275.3511396884918m \n",
      "Epoch:4-440/482, Train cross_entropy Loss: 0.008789384737610817, Cumulated Time: 279.8601784427961m \n",
      "Epoch:4-480/482, Train cross_entropy Loss: 0.02735283598303795, Cumulated Time: 284.4467928687731m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9209    0.8967    0.9087      1104\n",
      "           1     0.8007    0.8435    0.8215       543\n",
      "\n",
      "    accuracy                         0.8792      1647\n",
      "   macro avg     0.8608    0.8701    0.8651      1647\n",
      "weighted avg     0.8813    0.8792    0.8799      1647\n",
      "\n",
      "Epoch : 4, weighted avg f1-score: 87.994 Acc : 87.917 on Test_set, Spend:4.386 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:4 Completed, Total Train Loss:64.19231360522099, Test Loss:43.93559076124802, Spend 288.9314706206322m \n",
      "Epoch:5-0/482, Train cross_entropy Loss: 0.011867072433233261, Cumulated Time: 289.1129020531972m \n",
      "Epoch:5-40/482, Train cross_entropy Loss: 0.018148893490433693, Cumulated Time: 293.71659475564957m \n",
      "Epoch:5-80/482, Train cross_entropy Loss: 0.01771935261785984, Cumulated Time: 298.35858509937924m \n",
      "Epoch:5-120/482, Train cross_entropy Loss: 0.13000622391700745, Cumulated Time: 302.9111732641856m \n",
      "Epoch:5-160/482, Train cross_entropy Loss: 0.07037552446126938, Cumulated Time: 307.46596806844076m \n",
      "Epoch:5-200/482, Train cross_entropy Loss: 0.24848797917366028, Cumulated Time: 312.07687974770863m \n",
      "Epoch:5-240/482, Train cross_entropy Loss: 0.04072502627968788, Cumulated Time: 316.4895361383756m \n",
      "Epoch:5-280/482, Train cross_entropy Loss: 0.17399270832538605, Cumulated Time: 320.8185683051745m \n",
      "Epoch:5-320/482, Train cross_entropy Loss: 0.03864743188023567, Cumulated Time: 325.2689835588137m \n",
      "Epoch:5-360/482, Train cross_entropy Loss: 0.22915972769260406, Cumulated Time: 329.7005509654681m \n",
      "Epoch:5-400/482, Train cross_entropy Loss: 0.039497144520282745, Cumulated Time: 334.06281397342684m \n",
      "Epoch:5-440/482, Train cross_entropy Loss: 0.01167897880077362, Cumulated Time: 338.3769880414009m \n",
      "Epoch:5-480/482, Train cross_entropy Loss: 0.009479604661464691, Cumulated Time: 342.83582392533623m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9239    0.9121    0.9180      1104\n",
      "           1     0.8259    0.8471    0.8364       543\n",
      "\n",
      "    accuracy                         0.8907      1647\n",
      "   macro avg     0.8749    0.8796    0.8772      1647\n",
      "weighted avg     0.8915    0.8907    0.8911      1647\n",
      "\n",
      "Epoch : 5, weighted avg f1-score: 89.106 Acc : 89.071 on Test_set, Spend:4.232 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:5 Completed, Total Train Loss:47.82756771030836, Test Loss:43.60984221333638, Spend 347.1654371658961m \n",
      "Epoch:6-0/482, Train cross_entropy Loss: 0.25896206498146057, Cumulated Time: 347.3482492367427m \n",
      "Epoch:6-40/482, Train cross_entropy Loss: 0.025338416919112206, Cumulated Time: 351.6746384263039m \n",
      "Epoch:6-80/482, Train cross_entropy Loss: 0.12912538647651672, Cumulated Time: 356.149995692571m \n",
      "Epoch:6-120/482, Train cross_entropy Loss: 0.06948213279247284, Cumulated Time: 360.59501202106475m \n",
      "Epoch:6-160/482, Train cross_entropy Loss: 0.015258992090821266, Cumulated Time: 364.9166786472003m \n",
      "Epoch:6-200/482, Train cross_entropy Loss: 0.036617934703826904, Cumulated Time: 369.3117949644724m \n",
      "Epoch:6-240/482, Train cross_entropy Loss: 0.007820052094757557, Cumulated Time: 373.78452260891595m \n",
      "Epoch:6-280/482, Train cross_entropy Loss: 0.18376493453979492, Cumulated Time: 378.1637199719747m \n",
      "Epoch:6-320/482, Train cross_entropy Loss: 0.013768477365374565, Cumulated Time: 382.4609177748362m \n",
      "Epoch:6-360/482, Train cross_entropy Loss: 0.08003602921962738, Cumulated Time: 386.8689845482508m \n",
      "Epoch:6-400/482, Train cross_entropy Loss: 0.007522023748606443, Cumulated Time: 391.4100318312645m \n",
      "Epoch:6-440/482, Train cross_entropy Loss: 0.016371777281165123, Cumulated Time: 395.78102231820424m \n",
      "Epoch:6-480/482, Train cross_entropy Loss: 0.07070565968751907, Cumulated Time: 400.1183617115021m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9204    0.9221    0.9213      1104\n",
      "           1     0.8410    0.8379    0.8395       543\n",
      "\n",
      "    accuracy                         0.8944      1647\n",
      "   macro avg     0.8807    0.8800    0.8804      1647\n",
      "weighted avg     0.8943    0.8944    0.8943      1647\n",
      "\n",
      "Epoch : 6, weighted avg f1-score: 89.430 Acc : 89.435 on Test_set, Spend:4.199 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:6 Completed, Total Train Loss:37.25847112806514, Test Loss:42.71547299681697, Spend 404.41245078643163m \n",
      "Epoch:7-0/482, Train cross_entropy Loss: 0.013844328932464123, Cumulated Time: 404.5808664441109m \n",
      "Epoch:7-40/482, Train cross_entropy Loss: 0.018197303637862206, Cumulated Time: 408.9953268845876m \n",
      "Epoch:7-80/482, Train cross_entropy Loss: 0.015396478585898876, Cumulated Time: 413.39649647076925m \n",
      "Epoch:7-120/482, Train cross_entropy Loss: 0.012209801934659481, Cumulated Time: 417.8131570180257m \n",
      "Epoch:7-160/482, Train cross_entropy Loss: 0.012368863448500633, Cumulated Time: 422.30677732626594m \n",
      "Epoch:7-200/482, Train cross_entropy Loss: 0.01497271191328764, Cumulated Time: 426.8575391848882m \n",
      "Epoch:7-240/482, Train cross_entropy Loss: 0.005245755892246962, Cumulated Time: 431.3133786956469m \n",
      "Epoch:7-280/482, Train cross_entropy Loss: 0.03687135502696037, Cumulated Time: 435.77873016198475m \n",
      "Epoch:7-320/482, Train cross_entropy Loss: 0.015760937705636024, Cumulated Time: 440.3031269788742m \n",
      "Epoch:7-360/482, Train cross_entropy Loss: 0.13381609320640564, Cumulated Time: 444.852009499073m \n",
      "Epoch:7-400/482, Train cross_entropy Loss: 0.10042382031679153, Cumulated Time: 449.4800997455915m \n",
      "Epoch:7-440/482, Train cross_entropy Loss: 0.006152931600809097, Cumulated Time: 454.06122649908065m \n",
      "Epoch:7-480/482, Train cross_entropy Loss: 0.19934634864330292, Cumulated Time: 458.58479984203973m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9161    0.9293    0.9227      1104\n",
      "           1     0.8520    0.8269    0.8393       543\n",
      "\n",
      "    accuracy                         0.8956      1647\n",
      "   macro avg     0.8840    0.8781    0.8810      1647\n",
      "weighted avg     0.8949    0.8956    0.8952      1647\n",
      "\n",
      "Epoch : 7, weighted avg f1-score: 89.516 Acc : 89.557 on Test_set, Spend:4.258 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:7 Completed, Total Train Loss:26.61305591207929, Test Loss:45.12523901253007, Spend 462.93878477811813m \n",
      "Epoch:8-0/482, Train cross_entropy Loss: 0.4965685307979584, Cumulated Time: 463.1089439431826m \n",
      "Epoch:8-40/482, Train cross_entropy Loss: 0.005912957713007927, Cumulated Time: 467.5709008375804m \n",
      "Epoch:8-80/482, Train cross_entropy Loss: 0.008018632419407368, Cumulated Time: 472.1064307530721m \n",
      "Epoch:8-120/482, Train cross_entropy Loss: 0.01661706157028675, Cumulated Time: 476.6616853833199m \n",
      "Epoch:8-160/482, Train cross_entropy Loss: 0.18215423822402954, Cumulated Time: 481.2906152566274m \n",
      "Epoch:8-200/482, Train cross_entropy Loss: 0.008618833497166634, Cumulated Time: 486.0113964875539m \n",
      "Epoch:8-240/482, Train cross_entropy Loss: 0.005161380395293236, Cumulated Time: 490.59425444602965m \n",
      "Epoch:8-280/482, Train cross_entropy Loss: 0.011239499785006046, Cumulated Time: 495.13901788393656m \n",
      "Epoch:8-320/482, Train cross_entropy Loss: 0.004754076711833477, Cumulated Time: 499.7732452114423m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8-360/482, Train cross_entropy Loss: 0.02980760484933853, Cumulated Time: 504.3873777627945m \n",
      "Epoch:8-400/482, Train cross_entropy Loss: 0.015450303442776203, Cumulated Time: 509.0204149365425m \n",
      "Epoch:8-440/482, Train cross_entropy Loss: 0.01990298181772232, Cumulated Time: 513.5904533425967m \n",
      "Epoch:8-480/482, Train cross_entropy Loss: 0.008336919359862804, Cumulated Time: 518.1594310641289m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9236    0.9194    0.9215      1104\n",
      "           1     0.8376    0.8453    0.8414       543\n",
      "\n",
      "    accuracy                         0.8950      1647\n",
      "   macro avg     0.8806    0.8823    0.8815      1647\n",
      "weighted avg     0.8952    0.8950    0.8951      1647\n",
      "\n",
      "Epoch : 8, weighted avg f1-score: 89.508 Acc : 89.496 on Test_set, Spend:4.340 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:8 Completed, Total Train Loss:20.37041185516864, Test Loss:46.64920394890942, Spend 522.597555510203m \n"
     ]
    }
   ],
   "source": [
    "optimizer = BertAdam(model.parameters(), lr=config_learning_rate0,\n",
    "                     warmup=config_warmup_proportion, t_total=total_train_steps, weight_decay=config_l2_decay)\n",
    "\n",
    "train_start = time.time()\n",
    "global_step_th = int(len(train_examples) / batch_size /\n",
    "                     gradient_accumulation_steps * start_epoch)\n",
    "\n",
    "all_loss_list = {'train': [], 'test': []}\n",
    "all_f1_list = {'train': [], 'test': []}\n",
    "for epoch in range(start_epoch, total_train_epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if prev_save_step > -1:\n",
    "            if step <= prev_save_step:\n",
    "                continue\n",
    "        if prev_save_step > -1:\n",
    "            prev_save_step = -1\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "\n",
    "        _, logits = model(gcn_adj_list, gcn_swop_eye,\n",
    "                          input_ids, segment_ids, input_mask)\n",
    "\n",
    "        if config_loss_criterion == 'mse':\n",
    "            if do_softmax_before_mse:\n",
    "                logits = F.softmax(logits, -1)\n",
    "            loss = F.mse_loss(logits, y_prob)\n",
    "        else:\n",
    "            if loss_weight is None:\n",
    "                loss = F.cross_entropy(logits, label_ids)\n",
    "            else:\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, num_classes), label_ids, loss_weight)\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step_th += 1\n",
    "        if step % 40 == 0:\n",
    "            print(\"Epoch:{}-{}/{}, Train {} Loss: {}, Cumulated Time: {}m \".format(epoch, step,\n",
    "                  len(train_dataloader), config_loss_criterion, loss.item(), (time.time() - train_start)/60.0))\n",
    "\n",
    "    print('*' * 50)\n",
    "    test_loss, test_acc, perform_metrics = evaluate(model, gcn_adj_list, test_dataloader, epoch, 'Test_set')\n",
    "    all_loss_list['train'].append(train_loss)\n",
    "    all_loss_list['test'].append(test_loss)\n",
    "    all_f1_list['test'].append(perform_metrics)\n",
    "    print(\"Epoch:{} Completed, Total Train Loss:{}, Test Loss:{}, Spend {}m \".format(\n",
    "        epoch, train_loss, test_loss, (time.time() - train_start) / 60.0))\n",
    "\n",
    "    if perform_metrics > perform_metrics_prev:\n",
    "        to_save = {'epoch': epoch, 'model_state': model.state_dict(),\n",
    "                   'valid_acc': test_acc, 'lower_case': do_lower_case,\n",
    "                   'perform_metrics': perform_metrics}\n",
    "        torch.save(to_save, os.path.join(output_dir, model_file_save))\n",
    "\n",
    "        perform_metrics_prev = perform_metrics\n",
    "\n",
    "        valid_f1_best_epoch = epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ed9b61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished! Total Spend Time: 522.5977262576421\n",
      "Test Weighted F1: 89.516 at 7 Epoch.\n"
     ]
    }
   ],
   "source": [
    "print('Optimization Finished! Total Spend Time:', (time.time() - train_start)/60.0)\n",
    "print('Test Weighted F1: %.3f at %d Epoch.' % (100*perform_metrics_prev, valid_f1_best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec3f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
